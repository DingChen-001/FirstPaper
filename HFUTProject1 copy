处理起始目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1
处理时间: 2025-02-15 10:56:57

└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\config
  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\config\fusion_config.yaml
     文件内容:
  # 数据配置
data:
  train_path: "./data/processed/train"
  val_path: "./data/processed/val"
  test_path: "./data/processed/test"
  input_size: 256
  batch_size: 32
  num_workers: 4

# 特征提取器配置
dnf:
  model_path: "./models/pre-trained/diffusion_model.pth"
  enabled: True

dire:
  vae_path: "./models/pre-trained/vae.pth"
  enabled: True

lgrad:
  backbone: "resnet50"
  pretrained_weights: "./models/pre-trained/resnet_ssl.pth"
  gradient_layers: ["layer3", "layer4"]

ssp:
  patch_size: 64
  num_patches: 16

# 融合模型训练配置
fusion:
  num_heads: 8
  feature_dim: 256
  dropout: 0.1
  lr: 5e-5
  epochs: 100
  adversarial_epsilon: 0.03

# 优化器配置
optimizer:
  type: "AdamW"
  weight_decay: 0.01
  momentum: 0.9

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\config\__init__.py
     文件内容:
  import yaml
from pathlib import Path

def load_config(config_path):
    """加载YAML配置文件并转换为字典"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # 路径自动补全处理
    base_dir = Path(__file__).parent.parent
    for key in ['data', 'models']:
        if key in config:
            config[key] = {k: str(base_dir / v) for k, v in config[key].items()}
    
    return config

└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\metadata
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\metadata\dataset_description.json
       文件内容:
    

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\processed
    └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\processed\test
    └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\processed\train
    └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\processed\val
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\raw
└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\deploy
  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\deploy\convert_onnx.py
     文件内容:
  import torch
from models import CrossViTFusion
from utils.export import export_to_onnx

# 加载训练好的模型
model = CrossViTFusion(num_classes=2)
model.load_state_dict(torch.load("models/trained/fusion_model.pth"))

# 创建示例输入（符合各特征的尺寸要求）
sample_input = {
    'dnf': torch.randn(1, 256, 256, 256),
    'dire': torch.randn(1, 256, 256, 256),
    'lgrad': torch.randn(1, 256, 256, 256),
    'ssp': torch.randn(1, 256, 256, 256)
}

# 导出为ONNX
export_to_onnx(model, sample_input, "deploy/fusion_model.onnx")

└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\docs
  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\docs\CONTRIBUTING.md
     文件内容:
  

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\docs\README.md
     文件内容:
  # 运行流程

- 数据准备：将原始数据按目录结构放入data/raw/

- 单特征预训练：运行 python main.py --stage pretrain --config config/dnf_config.yaml

- 融合模型训练：运行 python main.py --stage fusion

- 性能评估：运行 python main.py --stage test

- 模型导出：执行 python deploy/convert_onnx.py


└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\experiments
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\experiments\ablations
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\experiments\template
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\experiments\template\report.md
       文件内容:
    # 实验报告：{实验名称}

## 1. 实验配置

- **日期**: {date}
- **硬件**: {GPU型号}
- **数据集**: {数据集名称}
- **关键参数**:

  ```yaml
  {关键配置参数}


└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\logs
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\logs\tensorboard
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\logs\training_logs
└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\pre-trained
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\pre-trained\crossvit.pth
       文件内容:
    

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\pre-trained\diffusion_model.pth
       文件内容:
    

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\pre-trained\resnet50.pth
       文件内容:
    

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\trained
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\trained\dire_model.pth
       文件内容:
    

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\trained\dnf_model.pth
       文件内容:
    

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\trained\lgrad_model.pth
       文件内容:
    

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\trained\ssp_model.pth
       文件内容:
    

└── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\requirements.txt
   文件内容:
torch==2.0.1
torchvision==0.15.2
diffusers==0.18.2
transformers==4.31.0
onnxruntime==1.15.1
fastapi==0.99.1
uvicorn==0.22.0
python-multipart==0.0.6
scikit-learn==1.3.0
einops==0.6.1
tensorboard==2.13.0
pyyaml==6.0.1

└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\datasets
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\datasets\augmentation.py
       文件内容:
    import random
import torchvision.transforms as T

class GenerativeAwareAugment:
    """
    面向生成图像检测的特化数据增强组合
    包含：
    - 频域滤波（模拟生成伪影）
    - 局部噪声注入
    - 色彩偏移
    """
    def __init__(self):
        # 空间增强
        self.spatial_aug = T.Compose([
            T.RandomHorizontalFlip(),
            T.RandomRotation(15),
            T.RandomResizedCrop(256, scale=(0.8, 1.0))
        ])
        
        # 频域增强
        self.freq_aug = T.RandomApply([
            T.GaussianBlur(kernel_size=5),
            T.GaussianBlur(kernel_size=3)
        ], p=0.5)
        
        # 噪声增强
        self.noise_levels = [0.01, 0.03, 0.05]

    def __call__(self, img_tensor):
        # 空间变换
        img = self.spatial_aug(img_tensor)
        
        # 频域滤波
        img = self.freq_aug(img)
        
        # 注入脉冲噪声
        if random.random() > 0.7:
            noise_mask = torch.rand_like(img) < random.choice(self.noise_levels)
            img = torch.where(noise_mask, torch.rand_like(img), img)
        
        return img

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\datasets\dataset.py
       文件内容:
    import os
import torch
from torch.utils.data import Dataset
from PIL import Image
from .preprocess import FeaturePreprocessor

class MultiFeatureDataset(Dataset):
    """
    支持多特征提取的数据集类
    功能：
    - 自动加载图像并应用不同特征提取器
    - 支持对抗样本动态生成
    """
    def __init__(self, root_dir, preprocessors, transform=None, is_train=True):
        """
        Args:
            root_dir: 数据集根目录（需包含real_images/和generated_images/）
            preprocessors: 字典，包含各特征提取器实例
            transform: 数据增强函数
        """
        self.real_dir = os.path.join(root_dir, 'real_images')
        self.fake_dir = os.path.join(root_dir, 'generated_images')
        self.real_images = [os.path.join(self.real_dir, f) for f in os.listdir(self.real_dir)]
        self.fake_images = [os.path.join(self.fake_dir, f) for f in os.listdir(self.fake_dir)]
        self.all_images = self.real_images + self.fake_images
        self.labels = [0]*len(self.real_images) + [1]*len(self.fake_images)
        
        self.preprocessors = preprocessors  # {'dnf': DNF实例, 'dire': DIRE实例...}
        self.transform = transform
        self.feature_align = FeaturePreprocessor()
        self.is_train = is_train

    def __len__(self):
        return len(self.all_images)

    def __getitem__(self, idx):
        img_path = self.all_images[idx]
        label = self.labels[idx]
        
        # 加载图像并转换为Tensor
        img = Image.open(img_path).convert('RGB')
        img_tensor = torchvision.transforms.ToTensor()(img)
        
        # 数据增强（训练时启用）
        if self.transform and self.is_train:
            img_tensor = self.transform(img_tensor)
        
        # 并行提取各特征
        features = {}
        for name, processor in self.preprocessors.items():
            with torch.set_grad_enabled(name == 'dnf'):  # 仅DNF需要梯度
                features[name] = processor(img_tensor.unsqueeze(0)).squeeze(0)
        
        # 特征对齐
        aligned_features = self.feature_align(features)
        return aligned_features, torch.tensor(label, dtype=torch.long)

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\datasets\preprocess.py
       文件内容:
    import torch
import torchvision.transforms as T

class FeaturePreprocessor:
    """
    统一不同特征的空间和通道维度
    关键功能：
    - 空间重采样（对齐DIRE高分辨率残差与SSP块）
    - 通道压缩（统一DNF/LGrad维度）
    """
    def __init__(self, target_size=256, feature_dims=256):
        # 空间对齐变换
        self.resize = T.Resize(target_size, antialias=True)
        
        # 通道压缩层（1x1卷积）
        self.dnf_adaptor = nn.Conv2d(1, feature_dims, 1)
        self.dire_adaptor = nn.Conv2d(3, feature_dims, 1)
        self.lgrad_adaptor = nn.Conv2d(64, feature_dims, 1)
        self.ssp_adaptor = nn.Conv2d(128, feature_dims, 1)

    def __call__(self, features: dict) -> dict:
        """
        输入: 各特征的原始输出字典
            {
                "dnf": [B, 1, 128, 128],
                "dire": [B, 3, 512, 512],
                "lgrad": [B, 64, 256, 256],
                "ssp": [B, 128, 64, 64]
            }
        输出: 对齐后的特征字典（空间和通道统一）
        """
        processed = {}
        # 处理DNF特征
        processed['dnf'] = self.dnf_adaptor(features['dnf'])
        
        # 处理DIRE特征：降采样到256x256
        dire_resized = self.resize(features['dire'])
        processed['dire'] = self.dire_adaptor(dire_resized)
        
        # 处理LGrad特征：无需调整大小，只压缩通道
        processed['lgrad'] = self.lgrad_adaptor(features['lgrad'])
        
        # 处理SSP特征：上采样到256x256
        ssp_up = F.interpolate(features['ssp'], size=256, mode='bilinear')
        processed['ssp'] = self.ssp_adaptor(ssp_up)
        
        return processed  # 所有特征变为[B, 256, 256, 256]
        def fgsm_attack(image, epsilon, data_grad):
            """
            FGSM对抗样本生成（用于增强训练鲁棒性）
            参考论文：Adversarial Examples for Generative Models
            """
            sign_data_grad = data_grad.sign()
            perturbed_image = image + epsilon * sign_data_grad
            return torch.clamp(perturbed_image, 0, 1)

class AdversarialAugment:
    def __init__(self, model, epsilon=0.03):
        self.model = model
        self.epsilon = epsilon
    
    def __call__(self, x):
        x.requires_grad = True
        outputs = self.model(x)
        loss = F.cross_entropy(outputs, torch.zeros_like(outputs))  # 欺骗模型
        loss.backward()
        perturbed = fgsm_attack(x, self.epsilon, x.grad.data)
        return perturbed.detach()

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features
    └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\backbone
      └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\backbone\diffusion_unet.py
         文件内容:
      

      └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\backbone\resnet_ssl.py
         文件内容:
      

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\lgrad.py
       文件内容:
    import torch
import torch.nn as nn

class LearnableGradient(nn.Module):
    """
    LGrad特征提取器：基于深度梯度学习的伪影检测
    论文核心：生成图像的梯度模式与真实图像存在差异
    """
    def __init__(self, pretrained_path='models/pre-trained/resnet50.pth'):
        super().__init__()
        # 加载预训练ResNet并提取中间梯度
        self.backbone = ResNet50(pretrained_path)
        self.grad_layers = ['layer3', 'layer4']
        
        # 梯度特征处理（参考lgrad.txt的GradientStream模块）
        self.grad_processor = nn.Sequential(
            nn.Conv2d(2048, 512, 1),
            nn.ReLU(),
            nn.Conv2d(512, 256, 3, padding=1)
        )
        
        # 注册梯度钩子
        self.gradients = {}
        for name, module in self.backbone.named_modules():
            if name in self.grad_layers:
                module.register_forward_hook(self.save_gradient)
    
    def save_gradient(self, module, input, output):
        """保存中间层的梯度"""
        def grad_hook(grad):
            self.gradients[module] = grad
        output.register_hook(grad_hook)
    
    def forward(self, x):
        # 前向传播获取梯度
        _ = self.backbone(x)
        
        # 提取并处理梯度特征
        grad_feats = []
        for layer in self.grad_layers:
            grad = self.gradients[layer]
            processed = self.grad_processor(grad)
            grad_feats.append(F.adaptive_avg_pool2d(processed, (256, 256)))
        
        return torch.cat(grad_feats, dim=1)  # [B, 512, 256, 256]

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\ssp.py
       文件内容:
    import torch
import torch.nn as nn

class SingleSimplePatch(nn.Module):
    """
    SSP特征提取器：基于局部图像块的统计特征
    论文核心：生成图像在局部块内具有统计异常
    """
    def __init__(self, patch_size=64, num_patches=16):
        super().__init__()
        self.patch_size = patch_size
        self.num_patches = num_patches
        
        # 特征编码器（参考源码ssp.txt的LightweightStatsNet）
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),
            nn.AdaptiveAvgPool2d(1)
        )
        
    def random_patch(self, x):
        """动态选择高方差区域块（源码改进版）"""
        # 计算局部方差图
        unfolded = F.unfold(x, kernel_size=16, stride=8)
        variances = torch.var(unfolded, dim=1)  # [B, H*W]
        
        # 选择高方差区域索引
        _, top_indices = torch.topk(variances, self.num_patches, dim=1)
        return top_indices

    def forward(self, x):
        """
        输入: x [B, 3, H, W]
        输出: SSP特征 [B, 128]
        """
        # 动态选择图像块
        indices = self.random_patch(x)
        
        # 提取块特征
        batch_features = []
        for b in range(x.size(0)):
            patches = []
            for idx in indices[b]:
                # 计算块位置
                h = (idx // (x.size(3)//8)) * 8
                w = (idx % (x.size(3)//8)) * 8
                patch = x[b, :, h:h+self.patch_size, w:w+self.patch_size]
                patches.append(patch)
            
            # 编码块特征
            patches = torch.stack(patches)  # [num_patches, C, H, W]
            features = self.encoder(patches).squeeze(-1).squeeze(-1)
            batch_features.append(features.mean(dim=0))  # 平均所有块
        
        return torch.stack(batch_features)

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\fusion_layers
    └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\fusion_layers\crossvit_utils
      └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\fusion_layers\crossvit_utils\cross_attention.py
         文件内容:
      

      └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\fusion_layers\crossvit_utils\patch_embed.py
         文件内容:
      

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\fusion_layers\cross_attention_fusion.py
       文件内容:
    import torch
import torch.nn as nn
from einops import rearrange

class CrossScaleAttention(nn.Module):
    """
    改进的跨尺度注意力机制（参考CrossViT论文第3.2节）
    核心改进：引入位置编码与通道注意力
    """
    def __init__(self, dim, num_heads=8, scale_ratio=0.5):
        super().__init__()
        self.num_heads = num_heads
        self.scale = (dim // num_heads) ** -0.5
        self.scale_ratio = scale_ratio
        
        # 位置编码
        self.pos_embed = nn.Parameter(torch.randn(1, dim, 1, 1))
        
        # 线性变换
        self.to_qkv = nn.Conv2d(dim, dim * 3, 1)
        self.channel_attn = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(dim, dim // 16, 1),
            nn.ReLU(),
            nn.Conv2d(dim // 16, dim, 1),
            nn.Sigmoid()
        )

    def forward(self, local_feat, global_feat):
        """
        输入:
            local_feat: 局部特征 [B, C, H, W]
            global_feat: 全局特征 [B, C, H*s, W*s]
        输出: 融合特征 [B, C, H, W]
        """
        B, C, H, W = local_feat.shape
        
        # 缩放全局特征到局部尺寸
        global_down = F.interpolate(global_feat, scale_factor=self.scale_ratio, mode='bilinear')
        
        # 合并特征并添加位置编码
        fused = local_feat + global_down + self.pos_embed
        
        # 通道注意力加权
        channel_weights = self.channel_attn(fused)
        fused = fused * channel_weights
        
        # 生成QKV
        qkv = self.to_qkv(fused).chunk(3, dim=1)
        q, k, v = map(lambda t: rearrange(t, 'b (h d) x y -> b h (x y) d', h=self.num_heads), qkv)
        
        # 注意力计算
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        
        # 特征聚合
        out = (attn @ v)
        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x=H, y=W)
        return out

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\fusion_layers\weighted_fusion.py
       文件内容:
    import torch
import torch.nn as nn
import torch.nn.functional as F

class DynamicFeatureFusion(nn.Module):
    """
    动态加权多特征融合模块
    功能：为DNF、DIRE、LGrad、SSP特征分配可学习权重
    论文参考：DNF论文中的动态特征选择策略
    """
    def __init__(self, num_features=4, hidden_dim=128):
        super().__init__()
        # 权重生成网络（输入为各特征的均值）
        self.weight_net = nn.Sequential(
            nn.Linear(num_features * hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_features)
        )
        
    def forward(self, features: dict) -> torch.Tensor:
        """
        输入: features字典包含各特征张量（已对齐为相同尺寸）
        输出: 加权融合后的特征 [B, C, H, W]
        """
        # 拼接各特征的均值统计量
        stats = [torch.mean(feat, dim=[1,2,3]) for feat in features.values()]
        stats = torch.cat(stats, dim=1)  # [B, num_features*C]
        
        # 生成归一化权重
        weights = F.softmax(self.weight_net(stats), dim=1)  # [B, 4]
        
        # 加权融合
        weighted_feats = []
        for i, (name, feat) in enumerate(features.items()):
            weighted_feats.append(feat * weights[:, i].view(-1,1,1,1))
        
        return torch.sum(torch.stack(weighted_feats), dim=0)  # 求和融合

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\losses
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\losses\feature_loss.py
       文件内容:
    import torch.nn as nn
import torch.nn.functional as F

class MultiScaleFeatureLoss(nn.Module):
    """
    多尺度特征对比损失（用于预训练特征提取器）
    论文参考：DNF论文公式(5)的改进版本
    """
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
        self.cos_sim = nn.CosineSimilarity(dim=2)

    def forward(self, feat_real, feat_fake):
        """
        输入: 
            feat_real: 真实图像特征 [B, D]
            feat_fake: 生成图像特征 [B, D]
        输出: 
            对比损失值
        """
        # 拼接所有特征
        features = torch.cat([feat_real, feat_fake], dim=0)  # [2B, D]
        
        # 计算相似度矩阵
        sim_matrix = self.cos_sim(features.unsqueeze(1), features.unsqueeze(0))  # [2B, 2B]
        sim_matrix /= self.temperature
        
        # 构建标签（对角线为匹配样本）
        labels = torch.arange(features.size(0)).to(features.device)
        
        # 交叉熵损失（对角线为正样本）
        loss = F.cross_entropy(sim_matrix, labels)
        return loss

class ReconstructionLoss(nn.Module):
    """DIRE特征专用的重建损失（L1 + SSIM）"""
    def __init__(self, alpha=0.8):
        super().__init__()
        self.alpha = alpha
        self.ssim = SSIM(window_size=11)  # 需实现SSIM计算

    def forward(self, original, reconstructed):
        l1_loss = F.l1_loss(original, reconstructed)
        ssim_loss = 1 - self.ssim(original, reconstructed)
        return self.alpha * l1_loss + (1 - self.alpha) * ssim_loss

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\losses\fusion_loss.py
       文件内容:
    import torch
import torch.nn as nn
import torch.nn.functional as F

class HierarchicalFusionLoss(nn.Module):
    """
    融合模型的多任务损失函数：
    - 分类交叉熵损失（主损失）
    - 特征一致性损失（辅助损失）
    - 注意力稀疏正则（防止过拟合）
    """
    def __init__(self, alpha=0.5, beta=0.1):
        super().__init__()
        self.alpha = alpha  # 一致性损失权重
        self.beta = beta    # 正则项权重
        self.ce_loss = nn.CrossEntropyLoss()
        
    def feature_consistency(self, feat_dict):
        """特征间余弦相似度一致性约束"""
        loss = 0
        keys = list(feat_dict.keys())
        # 计算所有特征对的两两相似度
        for i in range(len(keys)):
            for j in range(i+1, len(keys)):
                sim = F.cosine_similarity(feat_dict[keys[i]], feat_dict[keys[j]], dim=1)
                loss += torch.var(sim)  # 惩罚相似度波动
        return loss / len(keys)
    
    def attention_sparsity(self, attn_weights):
        """注意力权重稀疏性正则（参考论文公式12）"""
        return torch.mean(torch.sum(attn_weights**2, dim=-1))

    def forward(self, outputs, feat_dict, attn_weights, labels):
        main_loss = self.ce_loss(outputs, labels)
        consist_loss = self.feature_consistency(feat_dict)
        sparsity_loss = self.attention_sparsity(attn_weights)
        
        total_loss = main_loss + self.alpha*consist_loss + self.beta*sparsity_loss
        return {
            "total": total_loss,
            "ce": main_loss,
            "consistency": consist_loss,
            "sparsity": sparsity_loss
        }

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\main.py
     文件内容:
  import argparse
from train import train_stage1, train_stage2
from test import evaluate_model
from config import load_config

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--stage', type=str, required=True, 
                       choices=['pretrain', 'fusion', 'test'])
    parser.add_argument('--config', type=str, default="config/fusion_config.yaml")
    args = parser.parse_args()
    
    config = load_config(args.config)
    
    if args.stage == "pretrain":
        train_stage1(config)
    elif args.stage == "fusion":
        train_stage2(config)
    elif args.stage == "test":
        evaluate_model(config)

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\models
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\models\crossvit.py
       文件内容:
    import torch
from torch import nn
from einops import rearrange

class CrossAttentionBlock(nn.Module):
    """
    CrossViT的交叉注意力模块（参考crossvit.txt核心组件）
    输入：两个不同尺度的特征序列
    输出：交叉注意力增强后的融合特征
    """
    def __init__(self, dim, num_heads=8):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(dim, num_heads)
        
    def forward(self, x, context):
        # 交换序列维度与通道维度
        x = rearrange(x, "b c h w -> (h w) b c")
        context = rearrange(context, "b c h w -> (h w) b c")
        
        # 交叉注意力（以x为query，context为key/value）
        attn_out, _ = self.attn(
            query=self.norm1(x),
            key=self.norm1(context),
            value=self.norm1(context)
        )
        x = x + attn_out  # 残差连接
        return rearrange(x, "(h w) b c -> b c h w", h=int(x.shape[0]**0.5))

class CrossViTFusion(nn.Module):
    """
    多尺度CrossViT融合模型（支持两阶段层次化融合）
    """
    def __init__(self, in_dim=256, num_classes=2):
        super().__init__()
        # 局部分支（处理高频特征：SSP和LGrad）
        self.local_branch = CrossAttentionBlock(in_dim)
        # 全局分支（处理语义特征：DNF和DIRE）
        self.global_branch = CrossAttentionBlock(in_dim)
        
        # 分类头
        self.head = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(in_dim, num_classes)
        
    def forward(self, features: dict) -> torch.Tensor:
        # 提取各特征
        dnf = features['dnf']
        dire = features['dire']
        lgrad = features['lgrad']
        ssp = features['ssp']
        
        # 第一阶段：局部特征融合（SSP + LGrad）
        local_fused = self.local_branch(ssp, lgrad)
        
        # 第二阶段：全局特征融合（DNF + DIRE）
        global_fused = self.global_branch(dnf, dire)
        
        # 拼接融合结果
        final_feature = torch.cat([local_fused, global_fused], dim=1)
        return self.head(final_feature)

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\models\fusion_model.py
       文件内容:
    import torch
import torch.nn as nn
from .crossvit import CrossViT
from src.features import DNFExtractor, DIREExtractor, LGradExtractor, SSPExtractor

class FusionModel(nn.Module):
    def __init__(self, dnf_path, dire_path, lgrad_path, ssp_patch_size):
        super(FusionModel, self).__init__()
        self.dnf_extractor = DNFExtractor(dnf_path)
        self.dire_extractor = DIREExtractor(dire_path)
        self.lgrad_extractor = LGradExtractor(lgrad_path)
        self.ssp_extractor = SSPExtractor(ssp_patch_size)
        
        # 定义 CrossViT 融合层
        self.crossvit = CrossViT(embed_dim=256, num_heads=8, depth=3)
        
        # 定义分类器
        self.classifier = nn.Sequential(
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Linear(128, 2)
        )

    def forward(self, image):
        # 提取各个特征
        dnf_feat = self.dnf_extractor.compute_dnf(image)
        dire_feat = self.dire_extractor.compute_dire(image)
        lgrad_feat = self.lgrad_extractor.compute_lgrad(image)
        ssp_feat = self.ssp_extractor.compute_ssp(image)
        
        # 融合特征
        fused_feat = self.crossvit(dnf_feat, dire_feat)
        fused_feat = self.crossvit(fused_feat, lgrad_feat)
        fused_feat = self.crossvit(fused_feat, ssp_feat)
        
        # 分类
        output = self.classifier(fused_feat)
        return output

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\optimizers
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\optimizers\optimizer.py
       文件内容:
    from torch.optim import Adam, AdamW, SGD
from .scheduler import WarmupCosineDecay

def build_optimizer(model, config):
    """
    根据配置返回优化器与学习率调度器
    支持不同模块设置差异化的学习率
    """
    # 分离特征提取参数与融合参数
    feat_params = []
    fusion_params = []
    for name, param in model.named_parameters():
        if 'backbone' in name:
            feat_params.append(param)
        else:
            fusion_params.append(param)
    
    # 多参数组优化
    optimizer = AdamW([
        {'params': feat_params, 'lr': config.feat_lr},
        {'params': fusion_params, 'lr': config.fusion_lr}
    ], weight_decay=config.weight_decay)
    
    # 学习率调度
    scheduler = WarmupCosineDecay(
        optimizer, 
        warmup_epochs=config.warmup_epochs,
        total_epochs=config.total_epochs
    )
    return optimizer, scheduler

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\optimizers\scheduler.py
       文件内容:
    from torch.optim.lr_scheduler import _LRScheduler

class WarmupCosineDecay(_LRScheduler):
    """
    自定义学习率调度：线性热身+余弦衰减
    论文验证对GAN检测任务有效
    """
    def __init__(self, optimizer, warmup_epochs=5, total_epochs=100):
        self.warmup = warmup_epochs
        self.total = total_epochs
        super().__init__(optimizer)

    def get_lr(self):
        if self.last_epoch < self.warmup:
            # 线性热身
            return [base_lr * (self.last_epoch+1)/self.warmup 
                    for base_lr in self.base_lrs]
        else:
            # 余弦衰减
            progress = (self.last_epoch - self.warmup) / (self.total - self.warmup)
            return [base_lr * 0.5 * (1 + math.cos(math.pi * progress)) 
                    for base_lr in self.base_lrs]

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\test.py
     文件内容:
  import torch
from models import CrossViTFusion
from utils.metrics import DetectionEvaluator
from datasets import MultiFeatureDataset
from torch.utils.data import DataLoader

def evaluate_model(config):
    # 初始化模型
    model = CrossViTFusion(num_classes=2)
    model.load_state_dict(torch.load(config.model.checkpoint_path))
    model.eval()
    
    # 加载测试集
    test_dataset = MultiFeatureDataset(
        root_dir=config.data.test_path,
        preprocessors=config.preprocessors,
        is_train=False
    )
    test_loader = DataLoader(test_dataset, batch_size=config.test.batch_size)
    
    # 评估器
    evaluator = DetectionEvaluator()
    
    with torch.no_grad():
        for batch in test_loader:
            features, labels = batch
            start_time = time.time()
            outputs = model(features)
            infer_time = time.time() - start_time
            
            evaluator.update(outputs, labels, infer_time)
    
    metrics = evaluator.compute()
    print(f"测试结果：{metrics}")
    return metrics

if __name__ == "__main__":
    from config import load_config  # 假设有配置文件加载函数
    config = load_config("config/fusion_config.yaml")
    evaluate_model(config)

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\train.py
     文件内容:
  import torch
from models.crossvit import CrossViTFusion
from features import DNF, DIRE, LGrad, SSP
from datasets import FeatureDataset
from torch.utils.data import DataLoader

def train_stage1(config):
    """阶段1：单特征预训练"""
    # 初始化特征提取器
    dnf_extractor = DNF(pretrained_path=config.dnf.model_path)
    dire_extractor = DIRE(config.dire.vae_path)
    
    # 数据集与加载器
    dataset = FeatureDataset(root=config.data.path, 
                            preprocessors={'dnf': dnf_extractor, 'dire': dire_extractor})
    loader = DataLoader(dataset, batch_size=config.train.batch_size)
    
    # 训练循环（示例：DNF训练）
    optimizer = torch.optim.Adam(dnf_extractor.parameters(), lr=config.dnf.lr)
    for epoch in range(config.train.epochs):
        for batch in loader:
            images, labels = batch
            # 提取特征并计算损失
            features = dnf_extractor(images)
            loss = F.cross_entropy(features, labels)
            # 反向传播
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

def train_stage2(config):
    """阶段2:融合模型训练"""
    # 加载预训练特征提取器（冻结参数）
    extractors = {
        'dnf': DNF(pretrained_path=config.dnf.model_path).eval(),
        'dire': DIRE(config.dire.vae_path).eval(),
        'lgrad': LGrad(config.lgrad.weights_path).eval(),
        'ssp': SSP(config.ssp.patch_size).eval()
    }
    
    # 初始化融合模型
    fusion_model = CrossViTFusion(num_classes=2)
    
    # 数据加载（使用预处理对齐）
    dataset = FeatureDataset(root=config.data.path, 
                            preprocessors=extractors,
                            transform=AdversarialAugment(fusion_model))  # 对抗增强
    loader = DataLoader(dataset, batch_size=config.train.batch_size)
    
    # 优化器仅训练融合部分
    optimizer = torch.optim.AdamW(fusion_model.parameters(), lr=config.fusion.lr)
    
    # 训练循环
    for epoch in range(config.fusion.epochs):
        for batch in loader:
            features, labels = batch
            outputs = fusion_model(features)
            loss = F.cross_entropy(outputs, labels)
            # 梯度回传
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\distributed.py
       文件内容:
    import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

def setup_distributed():
    """初始化分布式训练环境"""
    dist.init_process_group(backend='nccl')
    local_rank = int(os.environ['LOCAL_RANK'])
    torch.cuda.set_device(local_rank)
    return local_rank

class DistributedWrapper:
    """分布式训练封装器（支持多GPU/多节点）"""
    def __init__(self, model, device_ids=None):
        self.local_rank = setup_distributed()
        self.model = DDP(model.to(self.local_rank), 
                        device_ids=[self.local_rank])

    def wrap_data_loader(self, dataset, batch_size):
        """分布式数据加载器"""
        sampler = DistributedSampler(dataset, 
                                   shuffle=True,
                                   num_replicas=dist.get_world_size(),
                                   rank=self.local_rank)
        return DataLoader(dataset, 
                         batch_size=batch_size,
                         sampler=sampler,
                         num_workers=4,
                         pin_memory=True)

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\export.py
       文件内容:
    import torch
import torch.onnx

def export_to_onnx(model, sample_input, output_path="model.onnx"):
    """
    将融合模型导出为ONNX格式（支持动态批量维度）
    输入示例：sample_input = {'dnf': torch.randn(1,256,256,256), ...}
    """
    # 将输入字典转换为元组（ONNX限制）
    input_names = list(sample_input.keys())
    input_tensors = tuple(sample_input.values())
    
    # 动态轴配置（批量维度可变化）
    dynamic_axes = {name: {0: 'batch_size'} for name in input_names}
    
    torch.onnx.export(
        model,
        input_tensors,
        output_path,
        input_names=input_names,
        output_names=["output"],
        dynamic_axes=dynamic_axes,
        opset_version=13
    )

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\logging.py
       文件内容:
    import time
from torch.utils.tensorboard import SummaryWriter

class TrainingLogger:
    """训练过程记录与可视化"""
    def __init__(self, log_dir):
        self.writer = SummaryWriter(log_dir=log_dir)
        self.start_time = time.time()
        self.step = 0
    
    def log_metrics(self, metrics_dict, phase='train'):
        """记录指标到TensorBoard"""
        for name, value in metrics_dict.items():
            self.writer.add_scalar(f'{phase}/{name}', value, self.step)
        
        # 计算累计时间
        elapsed = time.time() - self.start_time
        self.writer.add_scalar(f'{phase}/elapsed_time', elapsed, self.step)
        self.step += 1
    
    def log_model_graph(self, model, input_sample):
        """记录模型计算图"""
        self.writer.add_graph(model, input_sample)
    
    def close(self):
        self.writer.close()

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\metrics.py
       文件内容:
    import torch
from sklearn.metrics import accuracy_score, f1_score
import time

class DetectionEvaluator:
    """综合评估指标计算器"""
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.preds = []
        self.labels = []
        self.inference_times = []
    
    def update(self, logits: torch.Tensor, labels: torch.Tensor, infer_time: float):
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        self.preds.extend(preds)
        self.labels.extend(labels.cpu().numpy())
        self.inference_times.append(infer_time)
    
    def compute(self):
        """返回关键指标字典"""
        acc = accuracy_score(self.labels, self.preds)
        f1 = f1_score(self.labels, self.preds)
        avg_time = sum(self.inference_times) / len(self.inference_times)
        return {
            'Accuracy': round(acc, 4),
            'F1-Score': round(f1, 4),
            'Inference Time (ms)': round(avg_time * 1000, 2),
            'FPS': round(1 / avg_time, 1) if avg_time > 0 else 0
        }

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\profile.py
       文件内容:
    import torch.autograd.profiler as profiler

def profile_model(model, input_sample):
    """使用PyTorch Profiler分析模型计算开销"""
    with profiler.profile(
        activities=[profiler.ProfilerActivity.CUDA],
        profile_memory=True,
        with_stack=True
    ) as prof:
        model(**input_sample)
    
    print(prof.key_averages().table(
        sort_by="cuda_time_total", 
        row_limit=20
    ))

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\visualization.py
       文件内容:
    import matplotlib.pyplot as plt

def plot_feature_heatmaps(features_dict):
    """绘制各特征通道的激活热力图"""
    plt.figure(figsize=(15, 10))
    for idx, (name, feat) in enumerate(features_dict.items()):
        # 取第一个批次样本的均值
        avg_feat = feat[0].mean(dim=0).cpu().numpy()
        
        plt.subplot(2, 2, idx+1)
        plt.imshow(avg_feat, cmap='viridis')
        plt.title(f'{name} Feature Map')
        plt.colorbar()
    
    plt.tight_layout()
    plt.savefig('feature_heatmaps.png')
    
class SSIM(nn.Module):
    """
    Structure Similarity Index Metric
    用于DIRE特征的重建质量评估
    """
    def __init__(self, window_size=11, sigma=1.5):
        super().__init__()
        self.window = self._gaussian_window(window_size, sigma)
        self.window_size = window_size
        self.channel = 3  # 假设输入为RGB图像
        
    def _gaussian_window(self, size, sigma):
        """生成高斯卷积核"""
        coords = torch.arange(size).float() - size//2
        g = torch.exp(-(coords**2) / (2*sigma**2))
        g /= g.sum()
        return g.view(1, 1, -1) * g.view(1, -1, 1)  # 2D高斯核
    
    def forward(self, img1, img2):
        # 参数校验
        if img1.size() != img2.size():
            raise ValueError("Input images must have the same dimensions")
        
        # 扩展维度（批量、通道）
        B, C, H, W = img1.size()
        window = self.window.repeat(C, 1, 1, 1).to(img1.device)
        
        # 计算均值
        mu1 = F.conv2d(img1, window, padding=self.window_size//2, groups=C)
        mu2 = F.conv2d(img2, window, padding=self.window_size//2, groups=C)
        
        # 计算方差与协方差
        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1 * mu2
        
        sigma1_sq = F.conv2d(img1*img1, window, padding=self.window_size//2, groups=C) - mu1_sq
        sigma2_sq = F.conv2d(img2*img2, window, padding=self.window_size//2, groups=C) - mu2_sq
        sigma12 = F.conv2d(img1*img2, window, padding=self.window_size//2, groups=C) - mu1_mu2
        
        # SSIM计算公式
        C1 = (0.01 * 1)**2  # 假设动态范围[0,1]
        C2 = (0.03 * 1)**2
        
        ssim_map = ((2*mu1_mu2 + C1) * (2*sigma12 + C2)) / \
                   ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
        
        return ssim_map.mean()

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\validate.py
     文件内容:
  def validate_model(model, val_loader, criterion):
    """
    在验证集上评估模型性能，返回关键指标
    包含早停(early stopping)逻辑判断
    """
    model.eval()
    evaluator = DetectionEvaluator()
    
    with torch.no_grad():
        for batch in val_loader:
            features, labels = batch
            outputs = model(features)
            loss = criterion(outputs, labels)
            
            evaluator.update(outputs, labels, 0)  # 时间统计关闭
    
    metrics = evaluator.compute()
    metrics['val_loss'] = loss.item()
    return metrics

class EarlyStopper:
    """早停机制控制器"""
    def __init__(self, patience=5, delta=0.001):
        self.patience = patience
        self.delta = delta
        self.counter = 0
        self.best_loss = float('inf')
    
    def should_stop(self, current_loss):
        if current_loss < self.best_loss - self.delta:
            self.best_loss = current_loss
            self.counter = 0
        else:
            self.counter += 1
        return self.counter >= self.patience

