处理起始目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1
处理时间: 2025-02-15 16:55:30

└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\config
  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\config\crossvit.yaml
     文件内容:
  model:
  in_dims:
    dnf: 256
    dire: 256
    lgrad: 512 
    ssp: 128
  num_heads: 8
  scales: [16, 8]
  hidden_dim: 512

training:
  lr: 5e-5
  batch_size: 32
  num_epochs: 100

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\config\fusion_config.yaml
     文件内容:
  # 数据配置
data:
  train_path: "./data/processed/train"
  val_path: "./data/processed/val"
  test_path: "./data/processed/test"
  input_size: 256
  batch_size: 32
  num_workers: 4

# 特征提取器配置
dnf:
  model_path: "./models/pre-trained/diffusion_model.pth"
  enabled: True

dire:
  vae_path: "./models/pre-trained/vae.pth"
  enabled: True

lgrad:
  backbone: "resnet50"
  pretrained_weights: "./models/pre-trained/resnet_ssl.pth"
  gradient_layers: ["layer3", "layer4"]

ssp:
  patch_size: 64
  num_patches: 16

# 融合模型训练配置
fusion:
  num_heads: 8
  feature_dim: 256
  dropout: 0.1
  lr: 5e-5
  epochs: 100
  adversarial_epsilon: 0.03

# 优化器配置
optimizer:
  type: "AdamW"
  weight_decay: 0.01
  momentum: 0.9

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\config\main.yaml
     文件内容:
  data:
  root: "./data"
  img_size: 256
  batch_size: 32
  num_workers: 4

models:
  crossvit:
    config_path: "./config/crossvit.yaml"
    checkpoint: "./models/trained/crossvit.pth"
  dnf:
    checkpoint: "./models/pre-trained/ddnim.pth"
  dire:
    vae_path: "./models/pre-trained/vae.pth"

training:
  lr: 1e-4
  epochs: 100
  warmup: 5

evaluation:
  metrics: ["accuracy", "f1", "confusion_matrix"]

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\config\__init__.py
     文件内容:
  import yaml
from pathlib import Path

def load_config(config_path):
    """加载YAML配置文件并转换为字典"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # 路径自动补全处理
    base_dir = Path(__file__).parent.parent
    for key in ['data', 'models']:
        if key in config:
            config[key] = {k: str(base_dir / v) for k, v in config[key].items()}
    
    return config

└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\metadata
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\metadata\dataset_description.json
       文件内容:
    

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\processed
    └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\processed\test
    └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\processed\train
    └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\processed\val
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\data\raw
└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\deploy
  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\deploy\convert_onnx.py
     文件内容:
  import torch
from models import CrossViTFusion
from utils.export import export_to_onnx

# 加载训练好的模型
model = CrossViTFusion(num_classes=2)
model.load_state_dict(torch.load("models/trained/fusion_model.pth"))

# 创建示例输入（符合各特征的尺寸要求）
sample_input = {
    'dnf': torch.randn(1, 256, 256, 256),
    'dire': torch.randn(1, 256, 256, 256),
    'lgrad': torch.randn(1, 256, 256, 256),
    'ssp': torch.randn(1, 256, 256, 256)
}

# 导出为ONNX
export_to_onnx(model, sample_input, "deploy/fusion_model.onnx")

└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\docs
  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\docs\CONTRIBUTING.md
     文件内容:
  

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\docs\README.md
     文件内容:
  # 指令

## 运行流程

- 数据准备：将原始数据按目录结构放入data/raw/

- 单特征预训练：运行 python main.py --stage pretrain --config config/dnf_config.yaml

- 融合模型训练：运行 python main.py --stage fusion

- 性能评估：运行 python main.py --stage test

- 模型导出：执行 python deploy/convert_onnx.py

---

## 运行指令

### 分阶段训练

python main.py --stage pretrain --config config/dnf.yaml
python main.py --stage fusion --config config/crossvit.yaml

### 启用多GPU训练

torchrun --nproc_per_node=4 main.py --stage fusion --config config/crossvit.yaml


└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\experiments
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\experiments\ablations
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\experiments\template
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\experiments\template\report.md
       文件内容:
    # 实验报告：{实验名称}

## 1. 实验配置

- **日期**: {date}
- **硬件**: {GPU型号}
- **数据集**: {数据集名称}
- **关键参数**:

  ```yaml
  {关键配置参数}


└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\logs
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\logs\tensorboard
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\logs\training_logs
└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\pre-trained
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\pre-trained\crossvit.pth
       文件内容:
    

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\pre-trained\diffusion_model.pth
       文件内容:
    

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\pre-trained\resnet50.pth
       文件内容:
    

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\trained
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\trained\dire_model.pth
       文件内容:
    

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\trained\dnf_model.pth
       文件内容:
    

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\trained\lgrad_model.pth
       文件内容:
    

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\models\trained\ssp_model.pth
       文件内容:
    

└── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src
  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\datasets
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\datasets\augmentation.py
       文件内容:
    import random
import torchvision.transforms as T

class GenerativeAwareAugment:
    """
    面向生成图像检测的特化数据增强组合
    包含：
    - 频域滤波（模拟生成伪影）
    - 局部噪声注入
    - 色彩偏移
    """
    def __init__(self):
        # 空间增强
        self.spatial_aug = T.Compose([
            T.RandomHorizontalFlip(),
            T.RandomRotation(15),
            T.RandomResizedCrop(256, scale=(0.8, 1.0))
        ])
        
        # 频域增强
        self.freq_aug = T.RandomApply([
            T.GaussianBlur(kernel_size=5),
            T.GaussianBlur(kernel_size=3)
        ], p=0.5)
        
        # 噪声增强
        self.noise_levels = [0.01, 0.03, 0.05]

    def __call__(self, img_tensor):
        # 空间变换
        img = self.spatial_aug(img_tensor)
        
        # 频域滤波
        img = self.freq_aug(img)
        
        # 注入脉冲噪声
        if random.random() > 0.7:
            noise_mask = torch.rand_like(img) < random.choice(self.noise_levels)
            img = torch.where(noise_mask, torch.rand_like(img), img)
        
        return img

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\datasets\dataset.py
       文件内容:
    from torchvision.transforms import Compose, Resize, ToTensor, Normalize

class GANDataset(torch.utils.data.Dataset):
    def __init__(self, data_root, split='train', img_size=256):
        self.paths = self._load_paths(data_root, split)
        self.transform = Compose([
            Resize((img_size, img_size)),
            ToTensor(),
            Normalize(mean=[0.485, 0.456, 0.406], 
                     std=[0.229, 0.224, 0.225])
        ])
    
    def _load_paths(self, root, split):
        real_dir = f"{root}/processed/{split}/real_images"
        gen_dir = f"{root}/processed/{split}/generated_images"
        real_paths = [os.path.join(real_dir, f) for f in os.listdir(real_dir)]
        gen_paths = [os.path.join(gen_dir, f) for f in os.listdir(gen_dir)]
        return real_paths + gen_paths
    
    def __len__(self):
        return len(self.paths)
    
    def __getitem__(self, idx):
        img = Image.open(self.paths[idx]).convert('RGB')
        label = 0 if 'real_images' in self.paths[idx] else 1
        return self.transform(img), torch.tensor(label)

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\datasets\preprocess.py
       文件内容:
    import torch
import torchvision.transforms as T

class FeaturePreprocessor:
    """
    统一不同特征的空间和通道维度
    关键功能：
    - 空间重采样（对齐DIRE高分辨率残差与SSP块）
    - 通道压缩（统一DNF/LGrad维度）
    """
    def __init__(self, target_size=256, feature_dims=256):
        # 空间对齐变换
        self.resize = T.Resize(target_size, antialias=True)
        
        # 通道压缩层（1x1卷积）
        self.dnf_adaptor = nn.Conv2d(1, feature_dims, 1)
        self.dire_adaptor = nn.Conv2d(3, feature_dims, 1)
        self.lgrad_adaptor = nn.Conv2d(64, feature_dims, 1)
        self.ssp_adaptor = nn.Conv2d(128, feature_dims, 1)

    def __call__(self, features: dict) -> dict:
        """
        输入: 各特征的原始输出字典
            {
                "dnf": [B, 1, 128, 128],
                "dire": [B, 3, 512, 512],
                "lgrad": [B, 64, 256, 256],
                "ssp": [B, 128, 64, 64]
            }
        输出: 对齐后的特征字典（空间和通道统一）
        """
        processed = {}
        # 处理DNF特征
        processed['dnf'] = self.dnf_adaptor(features['dnf'])
        
        # 处理DIRE特征：降采样到256x256
        dire_resized = self.resize(features['dire'])
        processed['dire'] = self.dire_adaptor(dire_resized)
        
        # 处理LGrad特征：无需调整大小，只压缩通道
        processed['lgrad'] = self.lgrad_adaptor(features['lgrad'])
        
        # 处理SSP特征：上采样到256x256
        ssp_up = F.interpolate(features['ssp'], size=256, mode='bilinear')
        processed['ssp'] = self.ssp_adaptor(ssp_up)
        
        return processed  # 所有特征变为[B, 256, 256, 256]
        def fgsm_attack(image, epsilon, data_grad):
            """
            FGSM对抗样本生成（用于增强训练鲁棒性）
            参考论文：Adversarial Examples for Generative Models
            """
            sign_data_grad = data_grad.sign()
            perturbed_image = image + epsilon * sign_data_grad
            return torch.clamp(perturbed_image, 0, 1)

class AdversarialAugment:
    def __init__(self, model, epsilon=0.03):
        self.model = model
        self.epsilon = epsilon
    
    def __call__(self, x):
        x.requires_grad = True
        outputs = self.model(x)
        loss = F.cross_entropy(outputs, torch.zeros_like(outputs))  # 欺骗模型
        loss.backward()
        perturbed = fgsm_attack(x, self.epsilon, x.grad.data)
        return perturbed.detach()

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features
    └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\backbone
      └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\backbone\diffusion_unet.py
         文件内容:
      

      └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\backbone\resnet_ssl.py
         文件内容:
      

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\dim_reduction.py
       文件内容:
    import torch
import torch.nn as nn
from sklearn.decomposition import PCA

class HybridReducer(nn.Module):
    """
    混合降维策略：
    - 训练阶段：使用可学习的1x1卷积降维
    - 推理阶段：可选PCA降维（需提前拟合）
    - 支持多种归一化方法
    """
    def __init__(self, in_dim, out_dim=256, norm_type='bn', use_pca=False):
        super().__init__()
        # 可学习降维
        self.conv_reducer = nn.Sequential(
            nn.Conv2d(in_dim, out_dim, 1),
            nn.GELU()
        )
        
        # PCA降维器
        self.pca = PCA(n_components=out_dim) if use_pca else None
        self.use_pca = use_pca
        
        # 归一化层
        self.norm = {
            'bn': nn.BatchNorm2d(out_dim),
            'ln': nn.LayerNorm([out_dim, 1, 1]),
            'in': nn.InstanceNorm2d(out_dim)
        }[norm_type]

    def fit_pca(self, features):
        """离线拟合PCA"""
        flattened = features.view(-1, features.size(1)).cpu().numpy()
        self.pca.fit(flattened)
        
    def forward(self, x):
        if self.training or not self.use_pca:
            # 训练模式使用卷积降维
            reduced = self.conv_reducer(x)
        else:
            # 推理模式可选PCA
            x_flat = x.permute(0,2,3,1).view(-1, x.size(1))
            reduced = torch.tensor(self.pca.transform(x_flat.cpu())).to(x.device)
            reduced = reduced.view(x.size(0), x.size(2), x.size(3), -1).permute(0,3,1,2)
            
        return self.norm(reduced)

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\dire.py
       文件内容:
    # src/features/dire.py
class DIREFeature(nn.Module):
    def __init__(self, model_path="models/dire/diffusion_model"):
        super().__init__()
        # 加载预训练扩散模型
        self.pipe = DiffusionPipeline.from_pretrained(model_path)
        self.vae = self.pipe.vae
        self.vae.requires_grad_(False)

        # 多尺度残差计算
        self.res_blocks = nn.ModuleList([
            nn.Conv2d(3, 64, 5, stride=2),
            nn.Conv2d(64, 128, 3, stride=2)
        ])

    def compute_residual(self, x):
        """计算扩散重构残差"""
        # 编码到潜空间
        latents = self.vae.encode(x).latent_dist.sample()
        # 解码重建图像
        recon = self.vae.decode(latents).sample
        return torch.abs(x - recon) * 10  # 放大残差

    def forward(self, x):
        residual = self.compute_residual(x)
        features = []
        for block in self.res_blocks:
            residual = block(residual)
            features.append(F.adaptive_avg_pool2d(residual, 1))
        return torch.cat(features, dim=1)

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\dnf.py
       文件内容:
    # src/features/dnf.py
from diffusers import DDIMScheduler, UNet2DModel

class DiffusionNoiseFeature(nn.Module):
    def __init__(self, ddim_config_path="config/ddim_config.yaml"):
        super().__init__()
        # 加载DDIM模型配置
        with open(ddim_config_path) as f:
            config = yaml.safe_load(f)
        
        # 初始化DDIM组件
        self.scheduler = DDIMScheduler(
            num_train_timesteps=config['num_train_timesteps'],
            beta_schedule=config['beta_schedule']
        )
        self.unet = UNet2DModel.from_pretrained(config['model_path'])
        self.unet.requires_grad_(False)  # 冻结参数

        # 逆向过程噪声预测头
        self.noise_predictor = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.GroupNorm(8, 64),
            nn.Conv2d(64, 3, 3, padding=1)
        )

    def reverse_process(self, x, steps=100):
        """执行DDIM逆向过程"""
        x_prev = x.clone()
        for t in reversed(range(0, steps)):
            alpha_bar = self.scheduler.alphas_cumprod[t]
            pred_noise = self.unet(x_prev, t).sample
            # DDIM更新规则
            x_prev = (x_prev - (1 - alpha_bar)**0.5 * pred_noise) / alpha_bar**0.5
        return x_prev

    def forward(self, x):
        # 步骤1：执行完整逆向过程
        reversed_x = self.reverse_process(x)
        # 步骤2：计算预测噪声残差
        noise_residual = self.noise_predictor(x - reversed_x)
        return noise_residual.mean(dim=[2,3])  # 全局池化

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\lgrad.py
       文件内容:
    # src/features/lgrad.py
class GradientOperator(nn.Module):
    """自定义梯度特征计算层"""
    def __init__(self):
        super().__init__()
        # Sobel算子参数
        self.sobel_x = nn.Parameter(
            torch.tensor([[[1, 0, -1], [2, 0, -2], [1, 0, -1]]], dtype=torch.float32),
            requires_grad=False
        )
        self.sobel_y = nn.Parameter(
            torch.tensor([[[1, 2, 1], [0, 0, 0], [-1, -2, -1]]], dtype=torch.float32),
            requires_grad=False
        )

    def forward(self, x):
        # 计算梯度幅值和方向
        g_x = F.conv2d(x, self.sobel_x.repeat(x.shape[1],1,1,1))
        g_y = F.conv2d(x, self.sobel_y.repeat(x.shape[1],1,1,1))
        magnitude = torch.sqrt(g_x**2 + g_y**2)
        orientation = torch.atan2(g_y, g_x)
        return torch.cat([magnitude, orientation], dim=1)

class LGradFeature(nn.Module):
    def __init__(self, pretrained_path="models/pre-trained/resnet50.pth"):
        super().__init__()
        self.grad_op = GradientOperator()
        self.backbone = ResNet50(pretrained=True)
        
        # 注册梯度钩子
        self.gradients = {}
        def save_grad(name):
            def hook(grad):
                self.gradients[name] = grad
            return hook
        
        self.backbone.layer3.register_forward_hook(save_grad('layer3'))
        self.backbone.layer4.register_forward_hook(save_grad('layer4'))

    def forward(self, x):
        # 原始图像梯度特征
        raw_grad = self.grad_op(x)
        
        # 深度特征梯度
        _ = self.backbone(x)
        layer3_grad = self.gradients['layer3']
        layer4_grad = self.gradients['layer4']
        
        # 特征融合
        return torch.cat([
            raw_grad,
            F.interpolate(layer3_grad, scale_factor=2),
            F.interpolate(layer4_grad, scale_factor=4)
        ], dim=1)

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\features\ssp.py
       文件内容:
    # src/features/ssp.py
class PatchSelector(nn.Module):
    """动态高频补丁选择模块"""
    def __init__(self, patch_size=64, top_k=5):
        super().__init__()
        self.patch_size = patch_size
        self.top_k = top_k
        self.variance_conv = nn.Conv2d(3, 1, patch_size, stride=patch_size//2)

    def forward(self, x):
        # 计算局部方差图
        var_map = self.variance_conv(x**2) - (self.variance_conv(x))**2
        # 选择方差最大的k个区域
        B, _, H, W = var_map.shape
        _, indices = torch.topk(var_map.view(B, -1), self.top_k)
        return indices

class SSPFeature(nn.Module):
    def __init__(self, encoder_dim=128):
        super().__init__()
        self.selector = PatchSelector()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.InstanceNorm2d(32),
            nn.Conv2d(32, encoder_dim, 3, padding=1)
        )

    def extract_patches(self, x, indices):
        patches = []
        for b in range(x.size(0)):
            h = (indices[b] // x.size(3)) * (self.selector.patch_size // 2)
            w = (indices[b] % x.size(3)) * (self.selector.patch_size // 2)
            patches.append(x[b:b+1, :, h:h+self.selector.patch_size, w:w+self.selector.patch_size])
        return torch.cat(patches, dim=0)

    def forward(self, x):
        indices = self.selector(x)
        patches = self.extract_patches(x, indices)
        encoded = self.encoder(patches)
        return encoded.mean(dim=[2,3])  # 全局平均

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\fusion_layers
    └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\fusion_layers\crossvit_utils
      └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\fusion_layers\crossvit_utils\cross_attention.py
         文件内容:
      

      └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\fusion_layers\crossvit_utils\patch_embed.py
         文件内容:
      

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\fusion_layers\cross_attention_fusion.py
       文件内容:
    import torch
import torch.nn as nn
from einops import rearrange

class CrossScaleAttention(nn.Module):
    """
    改进的跨尺度注意力机制（参考CrossViT论文第3.2节）
    核心改进：引入位置编码与通道注意力
    """
    def __init__(self, dim, num_heads=8, scale_ratio=0.5):
        super().__init__()
        self.num_heads = num_heads
        self.scale = (dim // num_heads) ** -0.5
        self.scale_ratio = scale_ratio
        
        # 位置编码
        self.pos_embed = nn.Parameter(torch.randn(1, dim, 1, 1))
        
        # 线性变换
        self.to_qkv = nn.Conv2d(dim, dim * 3, 1)
        self.channel_attn = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(dim, dim // 16, 1),
            nn.ReLU(),
            nn.Conv2d(dim // 16, dim, 1),
            nn.Sigmoid()
        )

    def forward(self, local_feat, global_feat):
        """
        输入:
            local_feat: 局部特征 [B, C, H, W]
            global_feat: 全局特征 [B, C, H*s, W*s]
        输出: 融合特征 [B, C, H, W]
        """
        B, C, H, W = local_feat.shape
        
        # 缩放全局特征到局部尺寸
        global_down = F.interpolate(global_feat, scale_factor=self.scale_ratio, mode='bilinear')
        
        # 合并特征并添加位置编码
        fused = local_feat + global_down + self.pos_embed
        
        # 通道注意力加权
        channel_weights = self.channel_attn(fused)
        fused = fused * channel_weights
        
        # 生成QKV
        qkv = self.to_qkv(fused).chunk(3, dim=1)
        q, k, v = map(lambda t: rearrange(t, 'b (h d) x y -> b h (x y) d', h=self.num_heads), qkv)
        
        # 注意力计算
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        
        # 特征聚合
        out = (attn @ v)
        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x=H, y=W)
        return out

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\fusion_layers\weighted_fusion.py
       文件内容:
    class SmartFusion(nn.Module):
    def __init__(self, feat_dims, cfg):
        super().__init__()
        # 动态配置降维器
        self.reducers = nn.ModuleDict({
            name: HybridReducer(
                dim, 
                out_dim=cfg.fusion.reduced_dim,
                norm_type=cfg.norm_type,
                use_pca=cfg.use_pca
            ) for name, dim in feat_dims.items()
        })
        
        # 自适应权重融合
        self.weights = nn.ParameterDict({
            name: nn.Parameter(torch.ones(1)) for name in feat_dims.keys()
        })

    def forward(self, features):
        # 降维与归一化
        reduced = {name: reducer(feat) for name, (feat, reducer) in zip(features, self.reducers)}
        
        # 加权融合
        total_weight = sum([self.weights[name] for name in features.keys()])
        fused = sum([reduced[name] * (self.weights[name]/total_weight) for name in features.keys()])
        return fused

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\losses
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\losses\feature_loss.py
       文件内容:
    import torch.nn as nn
import torch.nn.functional as F

class MultiScaleFeatureLoss(nn.Module):
    """
    多尺度特征对比损失（用于预训练特征提取器）
    论文参考：DNF论文公式(5)的改进版本
    """
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
        self.cos_sim = nn.CosineSimilarity(dim=2)

    def forward(self, feat_real, feat_fake):
        """
        输入: 
            feat_real: 真实图像特征 [B, D]
            feat_fake: 生成图像特征 [B, D]
        输出: 
            对比损失值
        """
        # 拼接所有特征
        features = torch.cat([feat_real, feat_fake], dim=0)  # [2B, D]
        
        # 计算相似度矩阵
        sim_matrix = self.cos_sim(features.unsqueeze(1), features.unsqueeze(0))  # [2B, 2B]
        sim_matrix /= self.temperature
        
        # 构建标签（对角线为匹配样本）
        labels = torch.arange(features.size(0)).to(features.device)
        
        # 交叉熵损失（对角线为正样本）
        loss = F.cross_entropy(sim_matrix, labels)
        return loss

class ReconstructionLoss(nn.Module):
    """DIRE特征专用的重建损失（L1 + SSIM）"""
    def __init__(self, alpha=0.8):
        super().__init__()
        self.alpha = alpha
        self.ssim = SSIM(window_size=11)  # 需实现SSIM计算

    def forward(self, original, reconstructed):
        l1_loss = F.l1_loss(original, reconstructed)
        ssim_loss = 1 - self.ssim(original, reconstructed)
        return self.alpha * l1_loss + (1 - self.alpha) * ssim_loss

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\losses\fusion_loss.py
       文件内容:
    import torch
import torch.nn as nn
import torch.nn.functional as F

class HierarchicalFusionLoss(nn.Module):
    """
    融合模型的多任务损失函数：
    - 分类交叉熵损失（主损失）
    - 特征一致性损失（辅助损失）
    - 注意力稀疏正则（防止过拟合）
    """
    def __init__(self, alpha=0.5, beta=0.1):
        super().__init__()
        self.alpha = alpha  # 一致性损失权重
        self.beta = beta    # 正则项权重
        self.ce_loss = nn.CrossEntropyLoss()
        
    def feature_consistency(self, feat_dict):
        """特征间余弦相似度一致性约束"""
        loss = 0
        keys = list(feat_dict.keys())
        # 计算所有特征对的两两相似度
        for i in range(len(keys)):
            for j in range(i+1, len(keys)):
                sim = F.cosine_similarity(feat_dict[keys[i]], feat_dict[keys[j]], dim=1)
                loss += torch.var(sim)  # 惩罚相似度波动
        return loss / len(keys)
    
    def attention_sparsity(self, attn_weights):
        """注意力权重稀疏性正则（参考论文公式12）"""
        return torch.mean(torch.sum(attn_weights**2, dim=-1))

    def forward(self, outputs, feat_dict, attn_weights, labels):
        main_loss = self.ce_loss(outputs, labels)
        consist_loss = self.feature_consistency(feat_dict)
        sparsity_loss = self.attention_sparsity(attn_weights)
        
        total_loss = main_loss + self.alpha*consist_loss + self.beta*sparsity_loss
        return {
            "total": total_loss,
            "ce": main_loss,
            "consistency": consist_loss,
            "sparsity": sparsity_loss
        }

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\main.py
     文件内容:
  import argparse
from train import train_stage1, train_stage2
from test import evaluate_model
from config import load_config

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--stage', type=str, required=True, 
                       choices=['pretrain', 'fusion', 'test'])
    parser.add_argument('--config', type=str, default="config/fusion_config.yaml")
    args = parser.parse_args()
    
    config = load_config(args.config)
    
    if args.stage == "pretrain":
        train_stage1(config)
    elif args.stage == "fusion":
        train_stage2(config)
    elif args.stage == "test":
        evaluate_model(config)

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\models
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\models\crossvit.py
       文件内容:
    class CrossViT(nn.Module):
    """
    完整CrossViT架构，包含：
    - 双分支多尺度处理
    - 交叉注意力融合
    - 分类头适配
    """
    def __init__(self, cfg):
        super().__init__()
        # 分支1：小尺度处理
        self.branch_small = self._build_branch(
            img_size=cfg.img_size,
            patch_size=cfg.patch_sizes[0],
            embed_dim=cfg.embed_dim,
            depth=cfg.depth
        )
        
        # 分支2：大尺度处理
        self.branch_large = self._build_branch(
            img_size=cfg.img_size,
            patch_size=cfg.patch_sizes[1],
            embed_dim=cfg.embed_dim,
            depth=cfg.depth
        )
        
        # 交叉注意力模块
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=cfg.embed_dim,
            num_heads=cfg.num_heads
        )
        
        # 分类头
        self.head = nn.Sequential(
            nn.LayerNorm(cfg.embed_dim * 2),
            nn.Linear(cfg.embed_dim * 2, cfg.num_classes)
        )

    def _build_branch(self, img_size, patch_size, embed_dim, depth):
        return nn.Sequential(
            PatchEmbed(img_size, patch_size, 3, embed_dim),
            *[TransformerBlock(embed_dim, num_heads=8) for _ in range(depth)]
        )

    def forward(self, x):
        # 双分支处理
        feat_s = self.branch_small(x)  # [B, N, D]
        feat_l = self.branch_large(x)  # [B, M, D]
        
        # 交叉注意力
        feat_s, _ = self.cross_attn(feat_s, feat_l, feat_l)
        feat_l, _ = self.cross_attn(feat_l, feat_s, feat_s)
        
        # 分类特征聚合
        cls_feat = torch.cat([feat_s[:, 0], feat_l[:, 0]], dim=1)
        return self.head(cls_feat)
    @classmethod
    def from_pretrained(cls, config_path, model_path=None):
        """智能加载模型"""
        with open(config_path) as f:
            cfg = yaml.safe_load(f)
        
        model = cls(cfg['model'])
        if model_path:
            state_dict = torch.load(model_path, map_location='cpu')
            
            # 处理可能的键不匹配
            new_dict = {}
            for k, v in state_dict.items():
                new_k = k.replace("module.", "")  # 处理DP/DDP保存的权重
                new_dict[new_k] = v
            model.load_state_dict(new_dict)
        
        return model

    def serve(self, img_tensor):
        """生产环境服务接口"""
        with torch.no_grad():
            features = self.extract_features(img_tensor)
            return {
                'prob_fake': torch.softmax(self(features), dim=-1)[:, 1].cpu().numpy(),
                'features': features.cpu().numpy()
            }

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\models\fusion_model.py
       文件内容:
    import torch
import torch.nn as nn
from .crossvit import CrossViT
from src.features import DNFExtractor, DIREExtractor, LGradExtractor, SSPExtractor

class FusionModel(nn.Module):
    def __init__(self, dnf_path, dire_path, lgrad_path, ssp_patch_size):
        super(FusionModel, self).__init__()
        self.dnf_extractor = DNFExtractor(dnf_path)
        self.dire_extractor = DIREExtractor(dire_path)
        self.lgrad_extractor = LGradExtractor(lgrad_path)
        self.ssp_extractor = SSPExtractor(ssp_patch_size)
        
        # 定义 CrossViT 融合层
        self.crossvit = CrossViT(embed_dim=256, num_heads=8, depth=3)
        
        # 定义分类器
        self.classifier = nn.Sequential(
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Linear(128, 2)
        )

    def forward(self, image):
        # 提取各个特征
        dnf_feat = self.dnf_extractor.compute_dnf(image)
        dire_feat = self.dire_extractor.compute_dire(image)
        lgrad_feat = self.lgrad_extractor.compute_lgrad(image)
        ssp_feat = self.ssp_extractor.compute_ssp(image)
        
        # 融合特征
        fused_feat = self.crossvit(dnf_feat, dire_feat)
        fused_feat = self.crossvit(fused_feat, lgrad_feat)
        fused_feat = self.crossvit(fused_feat, ssp_feat)
        
        # 分类
        output = self.classifier(fused_feat)
        return output

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\optimizers
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\optimizers\optimizer.py
       文件内容:
    from torch.optim import Adam, AdamW, SGD
from .scheduler import WarmupCosineDecay

def build_optimizer(model, config):
    """
    根据配置返回优化器与学习率调度器
    支持不同模块设置差异化的学习率
    """
    # 分离特征提取参数与融合参数
    feat_params = []
    fusion_params = []
    for name, param in model.named_parameters():
        if 'backbone' in name:
            feat_params.append(param)
        else:
            fusion_params.append(param)
    
    # 多参数组优化
    optimizer = AdamW([
        {'params': feat_params, 'lr': config.feat_lr},
        {'params': fusion_params, 'lr': config.fusion_lr}
    ], weight_decay=config.weight_decay)
    
    # 学习率调度
    scheduler = WarmupCosineDecay(
        optimizer, 
        warmup_epochs=config.warmup_epochs,
        total_epochs=config.total_epochs
    )
    return optimizer, scheduler

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\optimizers\scheduler.py
       文件内容:
    from torch.optim.lr_scheduler import _LRScheduler

class WarmupCosineDecay(_LRScheduler):
    """
    自定义学习率调度：线性热身+余弦衰减
    论文验证对GAN检测任务有效
    """
    def __init__(self, optimizer, warmup_epochs=5, total_epochs=100):
        self.warmup = warmup_epochs
        self.total = total_epochs
        super().__init__(optimizer)

    def get_lr(self):
        if self.last_epoch < self.warmup:
            # 线性热身
            return [base_lr * (self.last_epoch+1)/self.warmup 
                    for base_lr in self.base_lrs]
        else:
            # 余弦衰减
            progress = (self.last_epoch - self.warmup) / (self.total - self.warmup)
            return [base_lr * 0.5 * (1 + math.cos(math.pi * progress)) 
                    for base_lr in self.base_lrs]

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\test.py
     文件内容:
  import torch
from models import CrossViTFusion
from utils.metrics import DetectionEvaluator
from datasets import MultiFeatureDataset
from torch.utils.data import DataLoader

def evaluate_model(config):
    # 初始化模型
    model = CrossViTFusion(num_classes=2)
    model.load_state_dict(torch.load(config.model.checkpoint_path))
    model.eval()
    
    # 加载测试集
    test_dataset = MultiFeatureDataset(
        root_dir=config.data.test_path,
        preprocessors=config.preprocessors,
        is_train=False
    )
    test_loader = DataLoader(test_dataset, batch_size=config.test.batch_size)
    
    # 评估器
    evaluator = DetectionEvaluator()
    
    with torch.no_grad():
        for batch in test_loader:
            features, labels = batch
            start_time = time.time()
            outputs = model(features)
            infer_time = time.time() - start_time
            
            evaluator.update(outputs, labels, infer_time)
    
    metrics = evaluator.compute()
    print(f"测试结果：{metrics}")
    return metrics

if __name__ == "__main__":
    from config import load_config  # 假设有配置文件加载函数
    config = load_config("config/fusion_config.yaml")
    evaluate_model(config)

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\train.py
     文件内容:
  import torch
from models.crossvit import CrossViTFusion
from features import DNF, DIRE, LGrad, SSP
from datasets import FeatureDataset
from torch.utils.data import DataLoader

def train_stage1(config):
    """阶段1：单特征预训练"""
    # 初始化特征提取器
    dnf_extractor = DNF(pretrained_path=config.dnf.model_path)
    dire_extractor = DIRE(config.dire.vae_path)
    
    # 数据集与加载器
    dataset = FeatureDataset(root=config.data.path, 
                            preprocessors={'dnf': dnf_extractor, 'dire': dire_extractor})
    loader = DataLoader(dataset, batch_size=config.train.batch_size)
    
    # 训练循环（示例：DNF训练）
    optimizer = torch.optim.Adam(dnf_extractor.parameters(), lr=config.dnf.lr)
    for epoch in range(config.train.epochs):
        for batch in loader:
            images, labels = batch
            # 提取特征并计算损失
            features = dnf_extractor(images)
            loss = F.cross_entropy(features, labels)
            # 反向传播
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

def train_stage2(config):
    """阶段2:融合模型训练"""
    # 加载预训练特征提取器（冻结参数）
    extractors = {
        'dnf': DNF(pretrained_path=config.dnf.model_path).eval(),
        'dire': DIRE(config.dire.vae_path).eval(),
        'lgrad': LGrad(config.lgrad.weights_path).eval(),
        'ssp': SSP(config.ssp.patch_size).eval()
    }
    
    # 初始化融合模型
    fusion_model = CrossViTFusion(num_classes=2)
    
    # 数据加载（使用预处理对齐）
    dataset = FeatureDataset(root=config.data.path, 
                            preprocessors=extractors,
                            transform=AdversarialAugment(fusion_model))  # 对抗增强
    loader = DataLoader(dataset, batch_size=config.train.batch_size)
    
    # 优化器仅训练融合部分
    optimizer = torch.optim.AdamW(fusion_model.parameters(), lr=config.fusion.lr)
    
    # 训练循环
    for epoch in range(config.fusion.epochs):
        for batch in loader:
            features, labels = batch
            outputs = fusion_model(features)
            loss = F.cross_entropy(outputs, labels)
            # 梯度回传
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

def train_epoch(model, loader, optimizer, cfg):
    model.train()
    evaluator = AdvancedEvaluator()
    
    for batch_idx, (data, target) in enumerate(loader):
        data, target = data.to(cfg.device), target.to(cfg.device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = F.cross_entropy(output, target)
        
        loss.backward()
        optimizer.step()
        
        evaluator.update(output, target)
        
        if batch_idx % cfg.log_interval == 0:
            metrics = evaluator.compute()
            print(f"Train Loss: {loss.item():.4f} | AUC: {metrics['ROC_AUC']:.3f}")
    
    return evaluator.compute()

  └── 目录: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\config.py
       文件内容:
    import yaml
from addict import Dict

def load_config(path):
    with open(path, 'r') as f:
        return Dict(yaml.safe_load(f))

def merge_config(base, override):
    """合并多层配置"""
    for k, v in override.items():
        if isinstance(v, dict) and k in base:
            merge_config(base[k], v)
        else:
            base[k] = v

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\distributed.py
       文件内容:
    import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

def setup_distributed():
    """初始化分布式训练环境"""
    dist.init_process_group(backend='nccl')
    local_rank = int(os.environ['LOCAL_RANK'])
    torch.cuda.set_device(local_rank)
    return local_rank

class DistributedWrapper:
    """分布式训练封装器（支持多GPU/多节点）"""
    def __init__(self, model, device_ids=None):
        self.local_rank = setup_distributed()
        self.model = DDP(model.to(self.local_rank), 
                        device_ids=[self.local_rank])

    def wrap_data_loader(self, dataset, batch_size):
        """分布式数据加载器"""
        sampler = DistributedSampler(dataset, 
                                   shuffle=True,
                                   num_replicas=dist.get_world_size(),
                                   rank=self.local_rank)
        return DataLoader(dataset, 
                         batch_size=batch_size,
                         sampler=sampler,
                         num_workers=4,
                         pin_memory=True)

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\export.py
       文件内容:
    import torch
import torch.onnx

def export_to_onnx(model, sample_input, output_path="model.onnx"):
    """
    将融合模型导出为ONNX格式（支持动态批量维度）
    输入示例：sample_input = {'dnf': torch.randn(1,256,256,256), ...}
    """
    # 将输入字典转换为元组（ONNX限制）
    input_names = list(sample_input.keys())
    input_tensors = tuple(sample_input.values())
    
    # 动态轴配置（批量维度可变化）
    dynamic_axes = {name: {0: 'batch_size'} for name in input_names}
    
    torch.onnx.export(
        model,
        input_tensors,
        output_path,
        input_names=input_names,
        output_names=["output"],
        dynamic_axes=dynamic_axes,
        opset_version=13
    )

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\logging.py
       文件内容:
    import time
from torch.utils.tensorboard import SummaryWriter

class TrainingLogger:
    """训练过程记录与可视化"""
    def __init__(self, log_dir):
        self.writer = SummaryWriter(log_dir=log_dir)
        self.start_time = time.time()
        self.step = 0
    
    def log_metrics(self, metrics_dict, phase='train'):
        """记录指标到TensorBoard"""
        for name, value in metrics_dict.items():
            self.writer.add_scalar(f'{phase}/{name}', value, self.step)
        
        # 计算累计时间
        elapsed = time.time() - self.start_time
        self.writer.add_scalar(f'{phase}/elapsed_time', elapsed, self.step)
        self.step += 1
    
    def log_model_graph(self, model, input_sample):
        """记录模型计算图"""
        self.writer.add_graph(model, input_sample)
    
    def close(self):
        self.writer.close()

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\metrics.py
       文件内容:
    from sklearn.metrics import roc_auc_score, average_precision_score

class AdvancedEvaluator:
    def __init__(self):
        self.probs = []
        self.labels = []
    
    def update(self, outputs, labels):
        self.probs.extend(torch.softmax(outputs, dim=1)[:, 1].cpu().numpy())
        self.labels.extend(labels.cpu().numpy())
    
    def compute(self):
        return {
            'ROC_AUC': roc_auc_score(self.labels, self.probs),
            'PR_AUC': average_precision_score(self.labels, self.probs),
            'Confidence_Hist': self._plot_hist()
        }
    
    def _plot_hist(self):
        import matplotlib.pyplot as plt
        plt.hist(
            [p for p, l in zip(self.probs, self.labels) if l == 0],
            bins=30, alpha=0.5, label='Real'
        )
        plt.hist(
            [p for p, l in zip(self.probs, self.labels) if l == 1],
            bins=30, alpha=0.5, label='Fake'
        )
        plt.legend()
        return plt.gcf()

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\profile.py
       文件内容:
    import torch.autograd.profiler as profiler

def profile_model(model, input_sample):
    """使用PyTorch Profiler分析模型计算开销"""
    with profiler.profile(
        activities=[profiler.ProfilerActivity.CUDA],
        profile_memory=True,
        with_stack=True
    ) as prof:
        model(**input_sample)
    
    print(prof.key_averages().table(
        sort_by="cuda_time_total", 
        row_limit=20
    ))

    └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\utils\visualization.py
       文件内容:
    import matplotlib.pyplot as plt

def plot_feature_heatmaps(features_dict):
    """绘制各特征通道的激活热力图"""
    plt.figure(figsize=(15, 10))
    for idx, (name, feat) in enumerate(features_dict.items()):
        # 取第一个批次样本的均值
        avg_feat = feat[0].mean(dim=0).cpu().numpy()
        
        plt.subplot(2, 2, idx+1)
        plt.imshow(avg_feat, cmap='viridis')
        plt.title(f'{name} Feature Map')
        plt.colorbar()
    
    plt.tight_layout()
    plt.savefig('feature_heatmaps.png')
    
class SSIM(nn.Module):
    """
    Structure Similarity Index Metric
    用于DIRE特征的重建质量评估
    """
    def __init__(self, window_size=11, sigma=1.5):
        super().__init__()
        self.window = self._gaussian_window(window_size, sigma)
        self.window_size = window_size
        self.channel = 3  # 假设输入为RGB图像
        
    def _gaussian_window(self, size, sigma):
        """生成高斯卷积核"""
        coords = torch.arange(size).float() - size//2
        g = torch.exp(-(coords**2) / (2*sigma**2))
        g /= g.sum()
        return g.view(1, 1, -1) * g.view(1, -1, 1)  # 2D高斯核
    
    def forward(self, img1, img2):
        # 参数校验
        if img1.size() != img2.size():
            raise ValueError("Input images must have the same dimensions")
        
        # 扩展维度（批量、通道）
        B, C, H, W = img1.size()
        window = self.window.repeat(C, 1, 1, 1).to(img1.device)
        
        # 计算均值
        mu1 = F.conv2d(img1, window, padding=self.window_size//2, groups=C)
        mu2 = F.conv2d(img2, window, padding=self.window_size//2, groups=C)
        
        # 计算方差与协方差
        mu1_sq = mu1.pow(2)
        mu2_sq = mu2.pow(2)
        mu1_mu2 = mu1 * mu2
        
        sigma1_sq = F.conv2d(img1*img1, window, padding=self.window_size//2, groups=C) - mu1_sq
        sigma2_sq = F.conv2d(img2*img2, window, padding=self.window_size//2, groups=C) - mu2_sq
        sigma12 = F.conv2d(img1*img2, window, padding=self.window_size//2, groups=C) - mu1_mu2
        
        # SSIM计算公式
        C1 = (0.01 * 1)**2  # 假设动态范围[0,1]
        C2 = (0.03 * 1)**2
        
        ssim_map = ((2*mu1_mu2 + C1) * (2*sigma12 + C2)) / \
                   ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
        
        return ssim_map.mean()

  └── 文件路径: S:\Notes\HFUT\Experiment\_228\HFUTProject1\src\validate.py
     文件内容:
  def validate_model(model, val_loader, criterion):
    """
    在验证集上评估模型性能，返回关键指标
    包含早停(early stopping)逻辑判断
    """
    model.eval()
    evaluator = DetectionEvaluator()
    
    with torch.no_grad():
        for batch in val_loader:
            features, labels = batch
            outputs = model(features)
            loss = criterion(outputs, labels)
            
            evaluator.update(outputs, labels, 0)  # 时间统计关闭
    
    metrics = evaluator.compute()
    metrics['val_loss'] = loss.item()
    return metrics

class EarlyStopper:
    """早停机制控制器"""
    def __init__(self, patience=5, delta=0.001):
        self.patience = patience
        self.delta = delta
        self.counter = 0
        self.best_loss = float('inf')
    
    def should_stop(self, current_loss):
        if current_loss < self.best_loss - self.delta:
            self.best_loss = current_loss
            self.counter = 0
        else:
            self.counter += 1
        return self.counter >= self.patience

