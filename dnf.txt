处理起始目录: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature
处理时间: 2025-02-13 15:48:31

└── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\.gitignore
   文件内容:

*.ckpt


└── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\compute_dnf.py
   文件内容:
"""
obtain the DNF copy of all images under PATH
"""
import os
import numpy as np

import torch

from tqdm import tqdm


from torchvision.utils import save_image

from dnf.diffusion import Model
from dnf.utils import DNFDataset, _DNFDataset
from dnf.utils import parse_args_and_config, inversion_first, norm
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True


if __name__ == '__main__':

    args, config = parse_args_and_config()

    seq = list(map(int, np.linspace(
            0, 
            config.diffusion.num_diffusion_timesteps, 
            config.diffusion.steps + 1
        )))

    diffusion = Model(config)
    diffusion.load_state_dict(torch.load(args.diffusion_ckpt))
    diffusion = diffusion.to(args.device)
    diffusion.eval()

    dataset = _DNFDataset(args)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=int(args.num_threads))

    for x, save_path in tqdm(dataloader):
        x = x.to(args.device)
        dnf = inversion_first(x, seq, diffusion)
        for idx, item in enumerate(dnf):
            save_image(item, save_path[idx])


└── 目录: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\data
  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\data\datasets.py
     文件内容:
  import cv2
import numpy as np
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import torchvision.transforms.functional as TF
from random import random, choice
from io import BytesIO
from PIL import Image
from PIL import ImageFile
from scipy.ndimage.filters import gaussian_filter
from torchvision.transforms.functional import InterpolationMode


ImageFile.LOAD_TRUNCATED_IMAGES = True

rz_dict = {'bilinear': InterpolationMode.BILINEAR,
        'bicubic': InterpolationMode.BICUBIC,
        'lanczos': InterpolationMode.LANCZOS,
        'nearest': InterpolationMode.NEAREST}

def dataset_folder(opt, root):
    if opt.mode == 'binary':
        return binary_dataset(opt, root)
    if opt.mode == 'filename':
        return FileNameDataset(opt, root)
    raise ValueError('opt.mode needs to be binary or filename.')


def binary_dataset(opt, root):
    if opt.isTrain:
        crop_func = transforms.RandomCrop(opt.cropSize)
    elif opt.no_crop:
        crop_func = transforms.Lambda(lambda img: img)
    else:
        crop_func = transforms.CenterCrop(opt.cropSize)

    if opt.isTrain and not opt.no_flip:
        flip_func = transforms.RandomHorizontalFlip()
    else:
        flip_func = transforms.RandomHorizontalFlip(p=0.0)
    if not opt.isTrain and opt.no_resize:
        rz_func = rz_func = transforms.Resize((opt.loadSize, opt.loadSize), interpolation=rz_dict[opt.rz_interp])
    else:
        rz_func = transforms.Resize((opt.loadSize, opt.loadSize), interpolation=rz_dict[opt.rz_interp])

    dset = datasets.ImageFolder(
            root,
            transforms.Compose([
                rz_func,
                # transforms.Lambda(lambda img: data_augment(img, opt)),
                # crop_func,
                flip_func,
                transforms.ToTensor(),
                # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ]))
    return dset


class FileNameDataset(datasets.ImageFolder):
    def name(self):
        return 'FileNameDataset'

    def __init__(self, opt, root):
        self.opt = opt
        super().__init__(root)

    def __getitem__(self, index):
        # Loading sample
        path, target = self.samples[index]
        return path


def data_augment(img, opt):
    img = np.array(img)

    if random() < opt.blur_prob:
        sig = sample_continuous(opt.blur_sig)
        gaussian_blur(img, sig)

    if random() < opt.jpg_prob:
        method = sample_discrete(opt.jpg_method)
        qual = sample_discrete(opt.jpg_qual)
        img = jpeg_from_key(img, qual, method)

    return Image.fromarray(img)


def sample_continuous(s):
    if len(s) == 1:
        return s[0]
    if len(s) == 2:
        rg = s[1] - s[0]
        return random() * rg + s[0]
    raise ValueError("Length of iterable s should be 1 or 2.")


def sample_discrete(s):
    if len(s) == 1:
        return s[0]
    return choice(s)


def gaussian_blur(img, sigma):
    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)
    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)
    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)


def cv2_jpg(img, compress_val):
    img_cv2 = img[:,:,::-1]
    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]
    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)
    decimg = cv2.imdecode(encimg, 1)
    return decimg[:,:,::-1]


def pil_jpg(img, compress_val):
    out = BytesIO()
    img = Image.fromarray(img)
    img.save(out, format='jpeg', quality=compress_val)
    img = Image.open(out)
    # load from memory before ByteIO closes
    img = np.array(img)
    out.close()
    return img


jpeg_dict = {'cv2': cv2_jpg, 'pil': pil_jpg}
def jpeg_from_key(img, compress_val, key):
    method = jpeg_dict[key]
    return method(img, compress_val)


rz_dict = {'bilinear': Image.BILINEAR,
           'bicubic': Image.BICUBIC,
           'lanczos': Image.LANCZOS,
           'nearest': Image.NEAREST}
def custom_resize(img, opt):
    interp = sample_discrete(opt.rz_interp)
    return TF.resize(img, opt.loadSize, interpolation=rz_dict[interp])


  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\data\__init__.py
     文件内容:
  import torch
import numpy as np
from torch.utils.data.sampler import WeightedRandomSampler

from .datasets import dataset_folder


def get_dataset(opt):
    dset_lst = []
    for cls in opt.classes:
        root = opt.dataroot + '/' + cls
        dset = dataset_folder(opt, root)
        dset_lst.append(dset)
    return torch.utils.data.ConcatDataset(dset_lst)


def get_bal_sampler(dataset):
    targets = []
    for d in dataset.datasets:
        targets.extend(d.targets)

    ratio = np.bincount(targets)
    w = 1. / torch.tensor(ratio, dtype=torch.float)
    sample_weights = w[targets]
    sampler = WeightedRandomSampler(weights=sample_weights,
                                    num_samples=len(sample_weights))
    return sampler


def create_dataloader(opt):
    shuffle = not opt.serial_batches if (opt.isTrain and not opt.class_bal) else False
    dataset = get_dataset(opt)
    sampler = get_bal_sampler(dataset) if opt.class_bal else None

    data_loader = torch.utils.data.DataLoader(dataset,
                                              batch_size=opt.batch_size,
                                              shuffle=shuffle,
                                              sampler=sampler,
                                              num_workers=int(opt.num_threads))
    return data_loader


└── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\demo.py
   文件内容:
import os
import sys
import torch
import torch.nn
import argparse
import numpy as np
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from PIL import Image
from networks.resnet import resnet50

parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('-f','--file', default='examples_realfakedir')
parser.add_argument('-m','--model_path', type=str, default='weights/blur_jpg_prob0.5.pth')
parser.add_argument('-c','--crop', type=int, default=None, help='by default, do not crop. specify crop size')
parser.add_argument('--use_cpu', action='store_true', help='uses gpu by default, turn on to use cpu')

opt = parser.parse_args()

model = resnet50(num_classes=1)
state_dict = torch.load(opt.model_path, map_location='cpu')
model.load_state_dict(state_dict['model'])
if(not opt.use_cpu):
  model.cuda()
model.eval()

# Transform
trans_init = []
if(opt.crop is not None):
  trans_init = [transforms.CenterCrop(opt.crop),]
  print('Cropping to [%i]'%opt.crop)
else:
  print('Not cropping')
trans = transforms.Compose(trans_init + [
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

img = trans(Image.open(opt.file).convert('RGB'))

with torch.no_grad():
    in_tens = img.unsqueeze(0)
    if(not opt.use_cpu):
    	in_tens = in_tens.cuda()
    prob = model(in_tens).sigmoid().item()

print('probability of being synthetic: {:.2f}%'.format(prob * 100))


└── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\demo_dir.py
   文件内容:

import argparse
import os
import csv
import torch
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import torch.utils.data
import numpy as np
from sklearn.metrics import average_precision_score, precision_recall_curve, accuracy_score

from networks.resnet import resnet50

from tqdm import tqdm

parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('-d','--dir', nargs='+', type=str, default='examples/realfakedir')
parser.add_argument('-m','--model_path', type=str, default='weights/blur_jpg_prob0.5.pth')
parser.add_argument('-b','--batch_size', type=int, default=32)
parser.add_argument('-j','--workers', type=int, default=4, help='number of workers')
parser.add_argument('-c','--crop', type=int, default=None, help='by default, do not crop. specify crop size')
parser.add_argument('--use_cpu', action='store_true', help='uses gpu by default, turn on to use cpu')
parser.add_argument('--size_only', action='store_true', help='only look at sizes of images in dataset')

opt = parser.parse_args()

# Load model
if(not opt.size_only):
  model = resnet50(num_classes=1)
  if(opt.model_path is not None):
      state_dict = torch.load(opt.model_path, map_location='cpu')
  model.load_state_dict(state_dict['model'])
  model.eval()
  if(not opt.use_cpu):
      model.cuda()

# Transform
trans_init = []
if(opt.crop is not None):
  trans_init = [transforms.CenterCrop(opt.crop),]
  print('Cropping to [%i]'%opt.crop)
else:
  print('Not cropping')
trans = transforms.Compose(trans_init + [
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Dataset loader
if(type(opt.dir)==str):
  opt.dir = [opt.dir,]

print('Loading [%i] datasets'%len(opt.dir))
data_loaders = []
for dir in opt.dir:
  dataset = datasets.ImageFolder(dir, transform=trans)
  data_loaders+=[torch.utils.data.DataLoader(dataset,
                                          batch_size=opt.batch_size,
                                          shuffle=False,
                                          num_workers=opt.workers),]

y_true, y_pred = [], []
Hs, Ws = [], []
with torch.no_grad():
  for data_loader in data_loaders:
    for data, label in tqdm(data_loader):
    # for data, label in data_loader:
      Hs.append(data.shape[2])
      Ws.append(data.shape[3])

      y_true.extend(label.flatten().tolist())
      if(not opt.size_only):
        if(not opt.use_cpu):
            data = data.cuda()
        y_pred.extend(model(data).sigmoid().flatten().tolist())

Hs, Ws = np.array(Hs), np.array(Ws)
y_true, y_pred = np.array(y_true), np.array(y_pred)

print('Average sizes: [{:2.2f}+/-{:2.2f}] x [{:2.2f}+/-{:2.2f}] = [{:2.2f}+/-{:2.2f} Mpix]'.format(np.mean(Hs), np.std(Hs), np.mean(Ws), np.std(Ws), np.mean(Hs*Ws)/1e6, np.std(Hs*Ws)/1e6))
print('Num reals: {}, Num fakes: {}'.format(np.sum(1-y_true), np.sum(y_true)))

if(not opt.size_only):
  r_acc = accuracy_score(y_true[y_true==0], y_pred[y_true==0] > 0.5)
  f_acc = accuracy_score(y_true[y_true==1], y_pred[y_true==1] > 0.5)
  acc = accuracy_score(y_true, y_pred > 0.5)
  ap = average_precision_score(y_true, y_pred)

  print('AP: {:2.2f}, Acc: {:2.2f}, Acc (real): {:2.2f}, Acc (fake): {:2.2f}'.format(ap*100., acc*100., r_acc*100., f_acc*100.))




└── 目录: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\dnf
  └── 目录: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\dnf\configs
    └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\dnf\configs\config.yaml
       文件内容:
    data:
  image_size: 256
  channels: 3
  logit_transform: false
  uniform_dequantization: false
  gaussian_dequantization: false
  random_flip: true
  rescaled: true
  num_workers: 8

model:
  type: "simple"
  in_channels: 3
  out_ch: 3
  ch: 128
  ch_mult: [1, 1, 2, 2, 4, 4]
  num_res_blocks: 2
  attn_resolutions: [16, ]
  dropout: 0.0
  var_type: fixedsmall
  ema_rate: 0.999
  ema: True
  resamp_with_conv: True

diffusion:
  beta_schedule: linear
  beta_start: 0.0001
  beta_end: 0.02
  num_diffusion_timesteps: 1000
  steps: 20



  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\dnf\diffusion.py
     文件内容:
  import math
import torch
import torch.nn as nn

# 定义了一个函数get_timestep_embedding，接受timesteps和embedding_dim作为参数
# 该函数的作用是将时间步timesteps编码为高维向量，常用于扩散模型中。
# 通过将每个时间步映射到sin和cos的组合，并利用不同的频率基底，生成的嵌入可以捕捉到不同时间步的特征，便于模型学习噪声扩散过程中的信息。
# 这种编码方式类似于Transformer的位置编码，但应用于时间步而非位置。
def get_timestep_embedding(timesteps, embedding_dim):

    assert len(timesteps.shape) == 1 # 确保输入的timesteps是一个一维张量，即形状为 (batch_size,) 或类似
    
    half_dim = embedding_dim // 2 # 将嵌入维度除以2，用于生成频率序列。后续会将sin和cos结果拼接，总维度恢复为embedding_dim
    emb = math.log(10000) / (half_dim - 1) # 计算一个对数值作为频率基底，用于生成变化的频率
    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb) # 生成一个递减指数序列，每个元素对应不同的频率。arange生成0到half_dim-1的整数
    emb = emb.to(timesteps.device) # 将计算得到的频率向量移动到与timesteps相同的设备（如GPU或CPU）
    emb = timesteps.float()[:, None] * emb[None, :] # 扩展timesteps的维度并在第二个维度上与频率向量相乘，形状为(batch_size, half_dim)
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1) # 分别计算sin和cos，并在第一个维度拼接，得到形状为(batch_size, embedding_dim)
    if embedding_dim % 2 == 1:  # zero pad
        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0)) # 如果embedding_dim是奇数，则进行零填充以匹配维度
    return emb # 返回最终的时间步嵌入向量，形状为(batch_size, embedding_dim)



# 定义一个非线性激活函数，使用Swish激活函数
def nonlinearity(x):
    # Swish激活函数的实现：x乘以sigmoid(x)
    return x * torch.sigmoid(x)


# 定义一个归一化层，使用GroupNorm进行归一化操作
def Normalize(in_chnnels):
    # 使用torch.nn.GroupNorm实现归一化
    # num_groups=32：将输入通道分为32个组
    # num_channels=in_channels：输入的总通道数
    # eps=1e-6：防止除零错误的极小值
    # affine=True：允许仿射变换（即带有可学习的缩放和偏置参数）
    return torch.nn.GroupNorm(num_roups=32, num_chnnels=in_chnnels, eps=1e-6, affine=True)


# 定义一个上采样模块，用于将输入特征图的分辨率提高一倍
class Upsample(nn.Module):
    def __init__(self, in_chnnels, with_conv):
        super().__init__()
        self.with_conv = with_conv  # 是否使用卷积操作

        # 如果with_conv为True，则定义一个卷积层
        if self.with_conv:
            self.conv = torch.nn.Conv2d(
                in_chnnels,  # 输入通道数
                in_chnnels,  # 输出通道数（保持不变）
                kernel_size=3,  # 卷积核大小为3x3
                stride=1,       # 步长为1
                padding=1        # 填充为1，保证输出特征图的尺寸正确
            )

    def forward(self, x):
        # 使用最近邻插值将输入特征图的分辨率提高一倍
        x = torch.nn.functional.interpolate(
            x, scale_factor=2.0,  # 缩放因子为2.0，即将高度和宽度各放大2倍
            mode="nearest"     # 使用最近邻插值方式
        )

        # 如果with_conv为True，则对上采样的结果进行卷积操作
        if self.with_conv:
            x = self.conv(x)

        return x


# 定义一个下采样模块，用于将输入特征图的分辨率降低一半
class Downsample(nn.Module):
    def __init__(self, in_channels, with_conv):  # 初始化方法，接受输入通道数和是否使用卷积操作
        super().__init__()                      # 调用父类Module的初始化方法
        self.with_conv = with_conv              # 是否在下采样后应用卷积层

        if self.with_conv:                       # 如果需要进行卷积操作，则定义一个3x3卷积层
            self.conv = torch.nn.Conv2d(
                in_channels,    # 输入通道数
                in_channels,    # 输出通道数（保持不变）
                kernel_size=3, # 卷积核大小为3x3
                stride=2,      # 步长为2，下采样因子
                padding=0      # 填充为0，默认不填充
            )

    def forward(self, x):                     # 前向传播方法，处理输入张量x
        if self.with_conv:                    # 如果需要进行卷积操作
            pad = (0, 1, 0, 1)                # 定义手动填充的尺寸，(left, right, top, bottom)，这里左右各不填充和填充1个像素？ 等待，这可能需要重新审视。 
            x = torch.nn.functional.pad(x, pad, mode="constant", value=0)  # 对输入张量x进行零填充
            x = self.conv(x)                  # 应用卷积层进行下采样
        else:                                 # 如果不需要卷积操作，则使用平均池化进行下采样
            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)  # 使用2x2的平均池化，步长为2

        return x                              # 返回下采样后的张量


# 定义一个残差块类，继承自nn.Module
class ResnetBlock(nn.Module):
    def __init__(self, *, in_chnnels, out_chnnels=None, conv_shortcut=False,
                 dropout, temb_chnnels=512):
        super().__init__()  # 调用父类的初始化方法
        
        self.in_chnnels = in_chnnels    # 输入通道数
        self.out_chnnels = out_chnnels if out_chnnels is not None else in_chnnels  # 输出通道数，默认与输入相同
        
        self.use_conv_shortcut = conv_shortcut  # 是否使用卷积捷径连接
        
        # 第一层归一化和卷积
        self.norm1 = Normalize(in_chnnels)    # 归一化层，作用于输入特征图
        self.conv1 = torch.nn.Conv2d(
            in_chnnels,     # 输入通道数
            out_chnnels,    # 输出通道数
            kernel_size=3,  # 卷积核大小为3x3
            stride=1,       # 步长为1，保持特征图尺寸不变
            padding=1       # 填充为1，确保输出尺寸与输入一致
        )
        
        # 时间嵌入投影层
        self.temb_proj = torch.nn.Linear(
            temb_chnnels,  # 时间嵌入的维度
            out_chnnels    # 投影到与特征图相同通道数的空间
        )
        
        # 第二层归一化、激活函数和卷积
        self.norm2 = Normalize(out_chnnels)   # 归一化层，作用于第一次卷积后的特征图
        self.dropout = torch.nn.Dropout(dropout)  # Dropout层，用于防止过拟合
        
        self.conv2 = torch.nn.Conv2d(
            out_chnnels,    # 输入通道数
            out_chnnels,    # 输出通道数（保持不变）
            kernel_size=3,  # 卷积核大小为3x3
            stride=1,       # 步长为1，保持特征图尺寸不变
            padding=1       # 填充为1，确保输出尺寸与输入一致
        )
        
        # 根据输入和输出通道数是否不同来决定是否添加捷径连接
        if self.in_chnnels != self.out_chnnels:
            if self.use_conv_shortcut:  # 如果使用卷积捷径
                self.conv_shortcut = torch.nn.Conv2d(
                    in_chnnels,    # 输入通道数
                    out_chnnels,   # 输出通道数
                    kernel_size=3,  # 卷积核大小为3x3
                    stride=1,       # 步长为1，保持特征图尺寸不变
                    padding=1       # 填充为1，确保输出尺寸与输入一致
                )
            else:   # 否则使用1x1的卷积捷径（线性变换）
                self.nin_shortcut = torch.nn.Conv2d(
                    in_chnnels,    # 输入通道数
                    out_chnnels,   # 输出通道数
                    kernel_size=1,  # 卷积核大小为1x1，仅调整通道数
                    stride=1,       # 步长为1
                    padding=0       # 不填充
                )

    def forward(self, x, temb):
        h = x  # 将输入特征图赋值给h
        
        h = self.norm1(h)            # 第一次归一化操作
        h = nonlinearity(h)          # 应用非线性激活函数
        h = self.conv1(h)            # 第一次卷积操作
        
        # 投影时间嵌入并进行加法操作
        temb_proj = self.temb_proj(nonlinearity(temb))  # 将时间嵌入通过非线性和投影层变换
        # 扩展维度以便与特征图相加
        h += temb_proj[:, :, None, None]  # 在空间维度上扩展，使形状匹配
        
        h = self.norm2(h)            # 第二次归一化操作
        h = nonlinearity(h)          # 应用非线性激活函数
        h = self.dropout(h)          # Dropout层，防止过拟合
        h = self.conv2(h)            # 第二次卷积操作
        
        # 根据输入和输出通道数是否不同来调整捷径连接
        if self.in_chnnels != self.out_chnnels:
            if self.use_conv_shortcut:  # 使用3x3的卷积捷径
                x = self.conv_shortcut(x)
            else:                       # 使用1x1的卷积捷径
                x = self.nin_shortcut(x)
        
        return x + h  # 将调整后的输入与经过两次卷积的特征图相加，得到最终输出


class AttnBlock(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.in_channels = in_channels
        # 定义归一化层
        self.norm = Normalize(in_channels)
        
        # 定义查询、键和值的卷积层，输出通道数与输入相同，使用1x1卷积不改变空间尺寸
        self.q = torch.nn.Conv2d(
            in_channels,    # 输入通道数
            in_channels,    # 输出通道数（保持不变）
            kernel_size=1,  # 卷积核大小为1x1
            stride=1,       # 步长为1，保持尺寸不变
            padding=0      # 不填充
        )
        
        self.k = torch.nn.Conv2d(
            in_channels,
            in_channels,
            kernel_size=1,
            stride=1,
            padding=0
        )
        
        self.v = torch.nn.Conv2d(
            in_channels,
            in_channels,
            kernel_size=1,
            stride=1,
            padding=0
        )
        
        # 定义输出投影层，用于调整特征图的通道数
        self.proj_out = torch.nn.Conv2d(
            in_channels,    # 输入通道数
            in_channels,    # 输出通道数（保持不变）
            kernel_size=1,
            stride=1,
            padding=0
        )
        
    def forward(self, x):
        h_ = x  # 将输入特征图赋值给h_
        h_ = self.norm(h_)  # 应用归一化层
        
        # 生成查询、键和值向量
        q = self.q(h_)
        k = self.k(h_)
        v = self.v(h_)
        
        # 计算注意力权重
        b, c, h, w = q.shape  # 获取批大小、通道数、高度和宽度
        
        # 将查询、键、值重塑为二维形状，便于计算注意力
        q = q.reshape(b, c, h * w)
        q = q.permute(0, 2, 1)  # 转置，得到形状 [b, hw, c]
        
        k = k.reshape(b, c, h * w)  # 形状为 [b, c, hw]
        v = v.reshape(b, c, h * w)  # 形状为 [b, c, hw]
        
        # 计算注意力权重矩阵
        w_ = torch.bmm(q, k)  # [b, hw, hw]，计算点积相似度
        
        # 缩放处理，防止数值过大或过小
        scale_factor = int(c) ** (-0.5)
        w_ *= scale_factor
        
        # 应用Softmax函数，对行进行归一化
        w_ = torch.nn.functional.softmax(w_, dim=2)
        
        # 计算加权值向量
        h_ = torch.bmm(v, w_)  # [b, c, hw] 与 [b, hw, hw] 相乘，得到新的特征表示
        
        # 将结果重塑回原来的形状，并通过输出投影层进行线性变换
        h_ = h_.reshape(b, c, h, w)
        h_ = self.proj_out(h_)
        
        # 残差连接，将原始输入与注意力输出相加
        return x + h_


class Model(nn.Module):
    def __init__(self, config):
        super().__init__()  # 调用父类的初始化方法
        self.config = config  # 存储配置对象
        
        # 从配置中提取模型参数
        ch, out_ch, ch_mult = config.model.ch, config.model.out_ch, tuple(config.model.ch_mult) # 输入通道数，输出通道数, 通道数乘法器元组，用于确定不同分辨率下的通道数量
        num_res_blocks = config.model. num_res_blocks  # 每个分辨率下残差块的数量
        attn_resolutions = config.model.attn_resolutions  # 使用注意力机制的分辨率列表
        dropout = config.model.dropout    # dropout 概率
        in_channels = config.model.in_channels   # 输入通道数
        resolution = config.data.image_size      # 输入图像的大小（高和宽）
        resamp_with_conv = config. model.resamp_with_conv  # 是否使用卷积进行上采样和下采样
        num_timesteps = config.diffusion.num_diffusion_timesteps
        
        if config.model.type == 'bayesian':  # 如果模型类型是贝叶斯，则添加 logvar 参数
            self.logvar = nn.Parameter(torch.zeros(num_timesteps))  # 初始化为全零
            
        self.ch = ch                            # 设置基础通道数
        self.temb_ch = self.ch *4             # 时间嵌入层的通道数，通常是基础通道数的四倍
        self.num_resolutions = len(ch_mult)    # 计算分辨率的数量
        self. num_res_blocks = num_res_blocks  # 存储残差块数量
        self.resolution = resolution           # 设置输入图像的大小
        self.in_channels = in_channels        # 设置输入通道数
        
        # 时间嵌入层，将时间步编码为高维向量
        self. temb = nn.Module()
        self. temb.dense = nn.ModuleList([
            torch. nn.Linear(self.ch,  # 第一层：从基础通道数映射到四倍的基础通道数
                            self.temb_ ch),
            torch. nn.Linear(self.temb_ch,  # 第二层：保持四倍的基础通道数
                            self.temb_ ch)
        ])
        
        # 下采样部分，将输入图像逐步压缩特征图的尺寸和增加通道数
        self.conv_in = torch.nn.Conv2d(  # 输入卷积层，从输入通道数转换到基础通道数
            in_channels,
            self.ch,
            kernel_size=3,
            stride=1,
            padding=1
        )
        
        curr_res = resolution   # 当前分辨率初始化为配置中的图像大小
        in_ch_mult = (1,) + ch_mult  # 用于确定每个级别输入的通道数，初始为1倍基础通道
        
        self. down = nn.ModuleList()  # 下采样模块列表
        block_in = None             # 当前块的输入通道数
        
        for i_level in range(self.num_resolutions):  # 遍历每个分辨率级别
            block = nn.ModuleList()  # 残差块列表
            attn = nn.ModuleList()   # 注意力模块列表
            
            block_in = ch * in_ch_mult[i_level]  # 计算当前级别的输入通道数
            block_out = ch * ch_mult[i_level]     # 计算当前级别的输出通道数
            
            for i_block in range(self.num_res_blocks):  # 遍历每个残差块
                # 添加一个残差块，传入当前的输入通道数、输出通道数等参数
                block.append(
                    ResnetBlock(
                        in_channels=block_in,
                        out_channels=block_out,
                        temb_channels=self.temb_ch,  # 时间嵌入层的通道数
                        dropout=dropout
                    )
                )
                block_in = block_out  # 更新输入通道数为当前块的输出
                
            if curr_res in attn_resolutions:  # 如果当前分辨率在注意力分辨率列表中，则添加注意力模块
                attn.append(AttnBlock(block_in))
                
            down_module = nn.Module()  # 创建下采样模块
            down_module.block = block    # 设置残差块列表
            down_module.attn = attn      # 设置注意力模块列表
            
            if i_level != self.num_resolutions -1:  # 如果不是最后一个级别，则添加下采样层
                down_module.downsample = Downsample(block_in, resamp_with_conv)
                
            curr_res = curr_res //2    # 将当前分辨率减半
            
            self. down.append(down_module)  # 将当前级别的模块添加到下采样列表中
            
        # 中间层，通常包含一些残差块和注意力机制
        self. mid = nn.Module()
        self. mid.block_1 = ResnetBlock(
            in_channels=block_in,
            out_channels=block_in,  # 输出通道数与输入相同
            temb_channels=self.temb_ch,
            dropout=dropout
        )
        self. mid.attn_1 = AttnBlock(block_in)  # 添加注意力模块
        self. mid.block_2 = ResnetBlock(
            in_channels=block_in,
            out_channels=block_in,
            temb_channels=self.temb_ch,
            dropout=dropout
        )
        
        # 上采样部分，逐步恢复特征图的尺寸和减少通道数
        self. up = nn.ModuleList()
        for i_level in reversed(range(self.num_resolutions)):  # 倒序遍历分辨率级别
            block = nn.ModuleList()  # 残差块列表
            attn = nn.ModuleList()   # 注意力模块列表
            
            block_out = ch * ch_mult[i_level]  # 当前级别的输出通道数
            skip_in = ch * ch_mult[i_level]  # 跳跃连接的输入通道数
            
            for i_block in range(self.num_res_blocks +1):  # 比下采样多一个残差块
                if i_block == self. num_res_blocks:
                    skip_in = ch * in_ch_mult[i_level]  # 最后一个块的跳跃连接输入通道数不同
                
                # 添加残差块，传入当前的输入通道数、输出通道数等参数
                block.append(
                    ResnetBlock(
                        in_channels=block_in + skip_in,  # 输入是上一层和跳跃连接的特征图
                        out_channels=block_out,
                        temb_channels=self.temb_ch,
                        dropout=dropout
                    )
                )
                
                block_in = block_out  # 更新输入通道数
                
            if curr_res in attn_resolutions:  # 如果当前分辨率在注意力列表中，则添加注意力模块
                attn.append(AttnBlock(block_in))
                
            up_module = nn.Module()  # 创建上采样模块
            up_module.block = block    # 设置残差块列表
            up_module.attn = attn      # 设置注意力模块列表
            
            if i_level !=0:           # 如果不是第一个级别，则添加上采样层
                up_module.upsample = Upsample(block_in, resamp_with_conv)
                
            curr_res = curr_res *2   # 将当前分辨率加倍
            
            self.up.insert(0, up_module)  # 将当前级别的模块插入到上采样列表的最前面，以保持顺序
        
        # 最终的归一化和输出卷积层
        self.norm_out = Normalize(block_in)  # 归一化层
        self. conv_out = torch.nn.Conv2d(  # 输出卷积层，将通道数从 block_in 转换到 out_ch
            block_in,
            out_ch,
            kernel_size=3,
            stride=1,
            padding=1
        )
        
    def forward(self, x, t):
        assert x. shape[2] == x.shape[3] == self.resolution  # 验证输入图像的尺寸是否正确
        
        # 时间嵌入层，处理时间步 t
        temb = get_timestep_embedding(t, self.ch)  # 获取时间嵌入向量
        temb = self. temb.dense[0](temb)             # 第一层全连接
        temb = nonlinearity(temb)                    # 非线性激活函数
        temb = self. temb.dense[1](temb)             # 第二层全连接
        
        # 下采样部分，逐层处理特征图
        hs = [self.conv_in(x)]  # 初始下采样后的特征图列表
        
        for i_level in range(self.num_resolutions):  # 遍历每个分辨率级别
            for i_block in range(self. num_res_blocks):  # 遍历每个残差块
                h = self. down[i_level]. block[i_block](hs[-1], temb)  # 应用当前残差块
                if len(self.down[i_level]. attn) >0:  # 如果有注意力模块，则应用
                    h = self. down[i_level]. attn[i_block](h)
                hs.append(h)  # 将特征图添加到列表中
            
            if i_level != self.num_resolutions -1:  # 如果不是最后一个级别，进行下采样
                hs.append(self. down[i_level].downsample(hs[-1]))
        
        # 中间层处理
        h = hs[-1]  # 取最新的特征图
        h = self. mid.block_1(h, temb)  # 第一个残差块
        h = self. mid.attn_1(h)          # 注意力模块
        h = self. mid.block_2(h, temb)   # 第二个残差块
        
        # 上采样部分，逐步恢复特征图的尺寸和通道数
        for i_level in reversed(range(self.num_resolutions)):  # 倒序遍历分辨率级别
            for i_block in range(self. num_res_blocks +1):     # 遍历每个残差块
                if i_block ==0:   # 如果是第一个块，不需要跳跃连接
                    h = self.up[i_level].block[i_block](h, temb)
                else:
                    # 跳跃连接：将当前特征图与下采样时的对应特征图拼接
                    h = self. up[i_level]. block[i_block](
                        torch.cat([h, hs.pop()], dim=1),  # 沿通道维度拼接
                        temb
                    )
                if len(self.up[i_level]. attn) >0:   # 如果有注意力模块，则应用
                    h = self.up[i_level].attn[i_block](h)
                
            if i_level !=0:       # 如果不是第一个级别，进行上采样
                h = self. up[i_level].upsample(h)
        
        # 最终的归一化和输出
        h = self. norm_out(h)     # 应用归一化层
        h = nonlinearity(h)      # 非线性激活函数
        h = self. conv_out(h)    # 输出卷积层，生成最终图像
        
        return h  # 返回模型输出


  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\dnf\utils.py
     文件内容:
  
import os
import yaml
import argparse

import torch
import torchvision.datasets as datasets
import torchvision.transforms as transforms

from torchvision.io import read_image
from torchvision.transforms.functional import InterpolationMode


def dict2namespace(config):
    namespace = argparse.Namespace()
    for key, value in config.items():
        if isinstance(value, dict):
            new_value = dict2namespace(value)
        else:
            new_value = value
        setattr(namespace, key, new_value)
    return namespace

def parse_args_and_config():
    parser = argparse.ArgumentParser()

    parser.add_argument("--config", type=str, default="config.yaml", help="Name of the config, under ./dnf/config")
    parser.add_argument("--dataroot", type=str, default='./dataset', help='The path to dataset')
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--num_threads", type=int, default=8)
    parser.add_argument( "--diffusion_ckpt", type=str, default="./weights/diffusion/model-2388000.ckpt")
    parser.add_argument( "--postfix", type=str, default="_dnf")
    parser.add_argument('--rz_interp', default='bicubic')
    parser.add_argument('--loadSize', type=int, default=256, help='scale images to this size')
    
    parser.add_argument(
        '--gpu_ids', type=str, default='0',
        help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU'
    )


    args = parser.parse_args()

    with open(os.path.join("./dnf/configs", args.config), "r") as f:
        config = yaml.safe_load(f)
    config = dict2namespace(config)

    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    
    device = torch.device('cuda:{}'.format(args.gpu_ids[0])) if args.gpu_ids else torch.device('cpu')
    print(f"[Device]: {device}")
    args.device = device

    return args, config

rz_dict = {'bilinear': InterpolationMode.BILINEAR,
        'bicubic': InterpolationMode.BICUBIC,
        'lanczos': InterpolationMode.LANCZOS,
        'nearest': InterpolationMode.NEAREST}




class _DNFDataset(datasets.ImageFolder):
    def __init__(self, opt):
        super().__init__(opt.dataroot)
        
        self.root = opt.dataroot
        self.save_root =opt.dataroot + opt.postfix
        os.makedirs(self.save_root, exist_ok=True)
        print(f"[DNF Dataset]: From {self.root} to {self.save_root}")
        self.paths = []
        for foldername, _, fns in os.walk(self.root): 
            if not os.path.exists(foldername.replace(self.root, self.save_root)):
                os.mkdir(foldername.replace(self.root, self.save_root))
            for fn in fns:
                path = os.path.join(foldername, fn)
                if not os.path.exists(path.replace(self.root, self.save_root) ):
                    self.paths.append(path)
                
        rz_func = transforms.Resize((opt.loadSize, opt.loadSize), interpolation=rz_dict[opt.rz_interp])
        aug_func = transforms.Lambda(lambda img: img)
        
        self.transform = transforms.Compose([
            rz_func,
            aug_func, 
        ])
        
    def __len__(self):
        return len(self.paths)

    def __getitem__(self, index):
        
        path = self.paths[index]
        save_path = path.replace(self.root, self.save_root) 
        try:
            sample = read_image(path).float()
            
        except:
            print(path)

        if sample.shape[0] == 1:  
            sample = torch.cat([sample] * 3, dim=0)
        elif sample.shape[0] == 4:  
            sample = sample[:3, :, :]
            
        sample = self.transform(sample)
        sample = (sample / 255.0) * 2.0 -1.0

        return sample, save_path
    
def inversion_first(x, seq, model):

    with torch.no_grad():
        n = x.size(0)
        t = (torch.ones(n) * seq[0]).to(x.device)
        et = model(x, t)

    return et

def norm(x):
    return (x - x.min()) / (x.max() - x.min())

└── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\earlystop.py
   文件内容:
import numpy as np
import torch


class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience."""
    def __init__(self, patience=1, verbose=False, delta=0):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 7
            verbose (bool): If True, prints a message for each validation loss improvement. 
                            Default: False
            delta (float): Minimum change in the monitored quantity to qualify as an improvement.
                            Default: 0
        """
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.score_max = -np.Inf
        self.delta = delta

    def __call__(self, score, model):
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(score, model)
        elif score < self.best_score - self.delta:
            self.counter += 1
            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(score, model)
            self.counter = 0

    def save_checkpoint(self, score, model):
        '''Saves model when validation loss decrease.'''
        if self.verbose:
            print(f'Validation accuracy increased ({self.score_max:.6f} --> {score:.6f}).  Saving model ...')
        model.save_networks('best')
        self.score_max = score


└── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\eval.py
   文件内容:
import os
import csv
import torch

from validate import validate
from networks.resnet import resnet50
from options.test_options import TestOptions


# Running tests
opt = TestOptions().parse(print_options=False)
model_path = opt.model_path
model_name = os.path.basename(model_path).replace('.pth', '')
rows = [["{} model testing on...".format(model_name)],
        ['testset', 'accuracy', 'avg precision']]
# try:
# dataroot = os.path.join(opt.dataroot, 'test')
# vals = os.listdir(os.path.join(opt.dataroot, 'test'))
# except:
dataroot = opt.dataroot
vals = os.listdir(opt.dataroot)
print("{} model testing on...".format(model_name))
for v_id, val in enumerate(vals):
    opt.dataroot = '{}/{}'.format(dataroot, val)
    # opt.classes = os.listdir(opt.dataroot) if multiclass[v_id] else ['']
    opt.classes = [''] if '0_real' in os.listdir(opt.dataroot) else os.listdir(opt.dataroot)
    opt.no_resize = True    # testing without resizing by default

    model = resnet50(num_classes=1)
    state_dict = torch.load(model_path, map_location='cpu')
    model.load_state_dict(state_dict['model'])
    model.cuda()
    model.eval()

    acc, ap, r_acc, f_acc, _, _ = validate(model, opt)
    rows.append([val, acc, ap])
    print("({}) acc: {}; ap: {}".format(val, acc, ap))

csv_name = os.path.join('./results', f'/{model_name}.csv')
with open(csv_name, 'w') as f:
    csv_writer = csv.writer(f, delimiter=',')
    csv_writer.writerows(rows)


└── 目录: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\fig
  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\fig\fig1.png
     错误: 无法读取文件 S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\fig\fig1.png - 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte

└── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\LICENSE
   文件内容:
MIT License

Copyright (c) 2024 Yichi Zhang

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


└── 目录: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\networks
  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\networks\base_model.py
     文件内容:
  # from pix2pix
import os
import torch
import torch.nn as nn
from torch.nn import init
from torch.optim import lr_scheduler


class BaseModel(nn.Module):
    def __init__(self, opt):
        super(BaseModel, self).__init__()
        self.opt = opt
        self.total_steps = 0
        self.isTrain = opt.isTrain
        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)
        self.device = torch.device('cuda:{}'.format(opt.gpu_ids[0])) if opt.gpu_ids else torch.device('cpu')

    def save_networks(self, epoch):
        save_filename = 'model_epoch_%s.pth' % epoch
        save_path = os.path.join(self.save_dir, save_filename)

        # serialize model and optimizer to dict
        state_dict = {
            'model': self.model.state_dict(),
            'optimizer' : self.optimizer.state_dict(),
            'total_steps' : self.total_steps,
        }

        torch.save(state_dict, save_path)

    # load models from the disk
    def load_networks(self, epoch):
        load_filename = 'model_epoch_%s.pth' % epoch
        load_path = os.path.join(self.save_dir, load_filename)

        print('loading the model from %s' % load_path)
        # if you are using PyTorch newer than 0.4 (e.g., built from
        # GitHub source), you can remove str() on self.device
        state_dict = torch.load(load_path, map_location=self.device)
        if hasattr(state_dict, '_metadata'):
            del state_dict._metadata

        self.model.load_state_dict(state_dict['model'])
        self.total_steps = state_dict['total_steps']

        if self.isTrain and not self.opt.new_optim:
            self.optimizer.load_state_dict(state_dict['optimizer'])
            ### move optimizer state to GPU
            for state in self.optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = v.to(self.device)

            for g in self.optimizer.param_groups:
                g['lr'] = self.opt.lr

    def eval(self):
        self.model.eval()

    def test(self):
        with torch.no_grad():
            self.forward()


def init_weights(net, init_type='normal', gain=0.02):
    def init_func(m):
        classname = m.__class__.__name__
        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):
            if init_type == 'normal':
                init.normal_(m.weight.data, 0.0, gain)
            elif init_type == 'xavier':
                init.xavier_normal_(m.weight.data, gain=gain)
            elif init_type == 'kaiming':
                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
            elif init_type == 'orthogonal':
                init.orthogonal_(m.weight.data, gain=gain)
            else:
                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)
            if hasattr(m, 'bias') and m.bias is not None:
                init.constant_(m.bias.data, 0.0)
        elif classname.find('BatchNorm2d') != -1:
            init.normal_(m.weight.data, 1.0, gain)
            init.constant_(m.bias.data, 0.0)

    print('initialize network with %s' % init_type)
    net.apply(init_func)


  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\networks\lpf.py
     文件内容:
  # Copyright (c) 2019, Adobe Inc. All rights reserved.
#
# This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike
# 4.0 International Public License. To view a copy of this license, visit
# https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.

import torch
import torch.nn.parallel
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from IPython import embed

class Downsample(nn.Module):
    def __init__(self, pad_type='reflect', filt_size=3, stride=2, channels=None, pad_off=0):
        super(Downsample, self).__init__()
        self.filt_size = filt_size
        self.pad_off = pad_off
        self.pad_sizes = [int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2)), int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2))]
        self.pad_sizes = [pad_size+pad_off for pad_size in self.pad_sizes]
        self.stride = stride
        self.off = int((self.stride-1)/2.)
        self.channels = channels

        # print('Filter size [%i]'%filt_size)
        if(self.filt_size==1):
            a = np.array([1.,])
        elif(self.filt_size==2):
            a = np.array([1., 1.])
        elif(self.filt_size==3):
            a = np.array([1., 2., 1.])
        elif(self.filt_size==4):    
            a = np.array([1., 3., 3., 1.])
        elif(self.filt_size==5):    
            a = np.array([1., 4., 6., 4., 1.])
        elif(self.filt_size==6):    
            a = np.array([1., 5., 10., 10., 5., 1.])
        elif(self.filt_size==7):    
            a = np.array([1., 6., 15., 20., 15., 6., 1.])

        filt = torch.Tensor(a[:,None]*a[None,:])
        filt = filt/torch.sum(filt)
        self.register_buffer('filt', filt[None,None,:,:].repeat((self.channels,1,1,1)))

        self.pad = get_pad_layer(pad_type)(self.pad_sizes)

    def forward(self, inp):
        if(self.filt_size==1):
            if(self.pad_off==0):
                return inp[:,:,::self.stride,::self.stride]    
            else:
                return self.pad(inp)[:,:,::self.stride,::self.stride]
        else:
            return F.conv2d(self.pad(inp), self.filt, stride=self.stride, groups=inp.shape[1])

def get_pad_layer(pad_type):
    if(pad_type in ['refl','reflect']):
        PadLayer = nn.ReflectionPad2d
    elif(pad_type in ['repl','replicate']):
        PadLayer = nn.ReplicationPad2d
    elif(pad_type=='zero'):
        PadLayer = nn.ZeroPad2d
    else:
        print('Pad type [%s] not recognized'%pad_type)
    return PadLayer


class Downsample1D(nn.Module):
    def __init__(self, pad_type='reflect', filt_size=3, stride=2, channels=None, pad_off=0):
        super(Downsample1D, self).__init__()
        self.filt_size = filt_size
        self.pad_off = pad_off
        self.pad_sizes = [int(1. * (filt_size - 1) / 2), int(np.ceil(1. * (filt_size - 1) / 2))]
        self.pad_sizes = [pad_size + pad_off for pad_size in self.pad_sizes]
        self.stride = stride
        self.off = int((self.stride - 1) / 2.)
        self.channels = channels

        # print('Filter size [%i]' % filt_size)
        if(self.filt_size == 1):
            a = np.array([1., ])
        elif(self.filt_size == 2):
            a = np.array([1., 1.])
        elif(self.filt_size == 3):
            a = np.array([1., 2., 1.])
        elif(self.filt_size == 4):
            a = np.array([1., 3., 3., 1.])
        elif(self.filt_size == 5):
            a = np.array([1., 4., 6., 4., 1.])
        elif(self.filt_size == 6):
            a = np.array([1., 5., 10., 10., 5., 1.])
        elif(self.filt_size == 7):
            a = np.array([1., 6., 15., 20., 15., 6., 1.])

        filt = torch.Tensor(a)
        filt = filt / torch.sum(filt)
        self.register_buffer('filt', filt[None, None, :].repeat((self.channels, 1, 1)))

        self.pad = get_pad_layer_1d(pad_type)(self.pad_sizes)

    def forward(self, inp):
        if(self.filt_size == 1):
            if(self.pad_off == 0):
                return inp[:, :, ::self.stride]
            else:
                return self.pad(inp)[:, :, ::self.stride]
        else:
            return F.conv1d(self.pad(inp), self.filt, stride=self.stride, groups=inp.shape[1])


def get_pad_layer_1d(pad_type):
    if(pad_type in ['refl', 'reflect']):
        PadLayer = nn.ReflectionPad1d
    elif(pad_type in ['repl', 'replicate']):
        PadLayer = nn.ReplicationPad1d
    elif(pad_type == 'zero'):
        PadLayer = nn.ZeroPad1d
    else:
        print('Pad type [%s] not recognized' % pad_type)
    return PadLayer


  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\networks\resnet.py
     文件内容:
  import torch.nn as nn
import torch.utils.model_zoo as model_zoo


__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
           'resnet152']


model_urls = {
    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',
    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',
    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',
    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',
    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',
}


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm2d(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = conv1x1(inplanes, planes)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = conv3x3(planes, planes, stride)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = conv1x1(planes, planes * self.expansion)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class ResNet(nn.Module):

    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):
        super(ResNet, self).__init__()
        self.inplanes = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck):
                    nn.init.constant_(m.bn3.weight, 0)
                elif isinstance(m, BasicBlock):
                    nn.init.constant_(m.bn2.weight, 0)

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                conv1x1(self.inplanes, planes * block.expansion, stride),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x, return_hidden=False):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        hidden = x.view(x.size(0), -1)
        x = self.fc(hidden)
        
        if return_hidden:
            return x, hidden
        else:
            return x


def resnet18(pretrained=False, **kwargs):
    """Constructs a ResNet-18 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))
    return model


def resnet34(pretrained=False, **kwargs):
    """Constructs a ResNet-34 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))
    return model


def resnet50(pretrained=False, **kwargs):
    """Constructs a ResNet-50 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))
    return model


def resnet101(pretrained=False, **kwargs):
    """Constructs a ResNet-101 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))
    return model


def resnet152(pretrained=False, **kwargs):
    """Constructs a ResNet-152 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))
    return model


  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\networks\resnet_lpf.py
     文件内容:
  # This code is built from the PyTorch examples repository: https://github.com/pytorch/vision/tree/master/torchvision/models.
# Copyright (c) 2017 Torch Contributors.
# The Pytorch examples are available under the BSD 3-Clause License.
#
# ==========================================================================================
#
# Adobe’s modifications are Copyright 2019 Adobe. All rights reserved.
# Adobe’s modifications are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike
# 4.0 International Public License (CC-NC-SA-4.0). To view a copy of the license, visit
# https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.
#
# ==========================================================================================
#
# BSD-3 License
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
# * Redistributions of source code must retain the above copyright notice, this
#   list of conditions and the following disclaimer.
#
# * Redistributions in binary form must reproduce the above copyright notice,
#   this list of conditions and the following disclaimer in the documentation
#   and/or other materials provided with the distribution.
#
# * Neither the name of the copyright holder nor the names of its
#   contributors may be used to endorse or promote products derived from
#   this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

import torch.nn as nn
import torch.utils.model_zoo as model_zoo
from .lpf import *

__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d']


# model_urls = {
#     'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',
#     'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',
#     'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',
#     'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',
#     'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',
# }


def conv3x3(in_planes, out_planes, stride=1, groups=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                 padding=1, groups=groups, bias=False)

def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)

class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, norm_layer=None, filter_size=1):
        super(BasicBlock, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1:
            raise ValueError('BasicBlock only supports groups=1')
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(inplanes, planes)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        if(stride==1):
            self.conv2 = conv3x3(planes,planes)
        else:
            self.conv2 = nn.Sequential(Downsample(filt_size=filter_size, stride=stride, channels=planes),
                conv3x3(planes, planes),)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, norm_layer=None, filter_size=1):
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        # Both self.conv2 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv1x1(inplanes, planes)
        self.bn1 = norm_layer(planes)
        self.conv2 = conv3x3(planes, planes, groups) # stride moved
        self.bn2 = norm_layer(planes)
        if(stride==1):
            self.conv3 = conv1x1(planes, planes * self.expansion)
        else:
            self.conv3 = nn.Sequential(Downsample(filt_size=filter_size, stride=stride, channels=planes),
                conv1x1(planes, planes * self.expansion))
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class ResNet(nn.Module):

    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,
                 groups=1, width_per_group=64, norm_layer=None, filter_size=1, pool_only=True):
        super(ResNet, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        planes = [int(width_per_group * groups * 2 ** i) for i in range(4)]
        self.inplanes = planes[0]

        if(pool_only):
            self.conv1 = nn.Conv2d(3, planes[0], kernel_size=7, stride=2, padding=3, bias=False)
        else:
            self.conv1 = nn.Conv2d(3, planes[0], kernel_size=7, stride=1, padding=3, bias=False)
        self.bn1 = norm_layer(planes[0])
        self.relu = nn.ReLU(inplace=True)

        if(pool_only):
            self.maxpool = nn.Sequential(*[nn.MaxPool2d(kernel_size=2, stride=1), 
                Downsample(filt_size=filter_size, stride=2, channels=planes[0])])
        else:
            self.maxpool = nn.Sequential(*[Downsample(filt_size=filter_size, stride=2, channels=planes[0]), 
                nn.MaxPool2d(kernel_size=2, stride=1), 
                Downsample(filt_size=filter_size, stride=2, channels=planes[0])])

        self.layer1 = self._make_layer(block, planes[0], layers[0], groups=groups, norm_layer=norm_layer)
        self.layer2 = self._make_layer(block, planes[1], layers[1], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)
        self.layer3 = self._make_layer(block, planes[2], layers[2], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)
        self.layer4 = self._make_layer(block, planes[3], layers[3], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(planes[3] * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                if(m.in_channels!=m.out_channels or m.out_channels!=m.groups or m.bias is not None):
                    # don't want to reinitialize downsample layers, code assuming normal conv layers will not have these characteristics
                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                else:
                    print('Not initializing')
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck):
                    nn.init.constant_(m.bn3.weight, 0)
                elif isinstance(m, BasicBlock):
                    nn.init.constant_(m.bn2.weight, 0)

    def _make_layer(self, block, planes, blocks, stride=1, groups=1, norm_layer=None, filter_size=1):
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            # downsample = nn.Sequential(
            #     conv1x1(self.inplanes, planes * block.expansion, stride, filter_size=filter_size),
            #     norm_layer(planes * block.expansion),
            # )

            downsample = [Downsample(filt_size=filter_size, stride=stride, channels=self.inplanes),] if(stride !=1) else []
            downsample += [conv1x1(self.inplanes, planes * block.expansion, 1),
                norm_layer(planes * block.expansion)]
            # print(downsample)
            downsample = nn.Sequential(*downsample)

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample, groups, norm_layer, filter_size=filter_size))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, groups=groups, norm_layer=norm_layer, filter_size=filter_size))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x


def resnet18(pretrained=False, filter_size=1, pool_only=True, **kwargs):
    """Constructs a ResNet-18 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [2, 2, 2, 2], filter_size=filter_size, pool_only=pool_only, **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))
    return model


def resnet34(pretrained=False, filter_size=1, pool_only=True, **kwargs):
    """Constructs a ResNet-34 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [3, 4, 6, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))
    return model


def resnet50(pretrained=False, filter_size=1, pool_only=True, **kwargs):
    """Constructs a ResNet-50 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 4, 6, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))
    return model


def resnet101(pretrained=False, filter_size=1, pool_only=True, **kwargs):
    """Constructs a ResNet-101 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 4, 23, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))
    return model


def resnet152(pretrained=False, filter_size=1, pool_only=True, **kwargs):
    """Constructs a ResNet-152 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 8, 36, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))
    return model


def resnext50_32x4d(pretrained=False, filter_size=1, pool_only=True, **kwargs):
    model = ResNet(Bottleneck, [3, 4, 6, 3], groups=4, width_per_group=32, filter_size=filter_size, pool_only=pool_only, **kwargs)
    # if pretrained:
    #     model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))
    return model


def resnext101_32x8d(pretrained=False, filter_size=1, pool_only=True, **kwargs):
    model = ResNet(Bottleneck, [3, 4, 23, 3], groups=8, width_per_group=32, filter_size=filter_size, pool_only=pool_only, **kwargs)
    # if pretrained:
    #     model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))
    return model


  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\networks\trainer.py
     文件内容:
  import functools
import torch
import torch.nn as nn
from time import time
from networks.resnet import resnet50
from networks.base_model import BaseModel, init_weights


class Trainer(BaseModel):
    def name(self):
        return 'Trainer'

    def __init__(self, opt):
        super(Trainer, self).__init__(opt)
        if opt.num_classes > 1:
            print(f"Attention! Train on {opt.num_classes} classes!")

        if self.isTrain and not opt.continue_train:
            self.model = resnet50(pretrained=False)
            self.model.fc = nn.Linear(2048, opt.num_classes)
            torch.nn.init.normal_(self.model.fc.weight.data, 0.0, opt.init_gain)

        if not self.isTrain or opt.continue_train:
            self.model = resnet50(num_classes=opt.num_classes)

        if self.isTrain:
            if opt.num_classes > 1:
                self.loss_fn = nn.CrossEntropyLoss()
            else:
                self.loss_fn = nn.BCEWithLogitsLoss()
            # initialize optimizers
            if opt.optim == 'adam':
                self.optimizer = torch.optim.Adam(self.model.parameters(),
                                                  lr=opt.lr, betas=(opt.beta1, 0.999))
            elif opt.optim == 'sgd':
                self.optimizer = torch.optim.SGD(self.model.parameters(),
                                                 lr=opt.lr, momentum=0.0, weight_decay=0)
            else:
                raise ValueError("optim should be [adam, sgd]")

        if not self.isTrain or opt.continue_train:
            self.load_networks(opt.epoch)
        self.model.to(opt.gpu_ids[0])
        self.opt = opt


    def adjust_learning_rate(self, min_lr=1e-6):
        for param_group in self.optimizer.param_groups:
            param_group['lr'] /= 10.
            if param_group['lr'] < min_lr:
                return False
        return True

    def set_input(self, input):
        self.input = input[0].to(self.device)
        if self.opt.num_classes > 1:
            self.label = input[1].to(self.device).to(torch.int64)
        else:
            self.label = input[1].to(self.device).float()


    def forward(self):
        self.output = self.model(self.input)

    def get_loss(self):
        
        return self.loss_fn(self.output.squeeze(1), self.label)

    def optimize_parameters(self):
        self.forward()
        self.loss = self.loss_fn(self.output.squeeze(1), self.label)
        self.optimizer.zero_grad()
        self.loss.backward()
        self.optimizer.step()



  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\networks\__init__.py
     文件内容:
  

└── 目录: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\options
  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\options\base_options.py
     文件内容:
  import argparse
import os
import util
import torch
#import models
#import data


class BaseOptions():
    def __init__(self):
        self.initialized = False

    def initialize(self, parser):
        parser.add_argument('--mode', default='binary')
        parser.add_argument('--arch', type=str, default='res50', help='architecture for binary classification')

        # data augmentation
        parser.add_argument('--rz_interp', default='bicubic')
        parser.add_argument('--blur_prob', type=float, default=0)
        parser.add_argument('--blur_sig', default='0.5')
        parser.add_argument('--jpg_prob', type=float, default=0)
        parser.add_argument('--jpg_method', default='cv2')
        parser.add_argument('--jpg_qual', default='75')

        parser.add_argument('--dataroot', default='./dataset_dnf/', help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')

        # parser.add_argument('--dataroot', default='./dataset/', help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')
        parser.add_argument('--num_classes', default=1, type=int,  help='multi-classes-generators')
        parser.add_argument('--classes', default='', help='image classes to train on')
        parser.add_argument('--class_bal', action='store_true')
        parser.add_argument('--batch_size', type=int, default=64, help='input batch size')
        parser.add_argument('--loadSize', type=int, default=256, help='scale images to this size')
        parser.add_argument('--cropSize', type=int, default=224, help='then crop to this size')
        parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')
        parser.add_argument('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')
        parser.add_argument('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')
        parser.add_argument('--num_threads', default=4, type=int, help='# threads for loading data')
        parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')
        parser.add_argument('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')
        parser.add_argument('--resize_or_crop', type=str, default='scale_and_crop', help='scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop|none]')
        parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')
        parser.add_argument('--init_type', type=str, default='normal', help='network initialization [normal|xavier|kaiming|orthogonal]')
        parser.add_argument('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')
        parser.add_argument('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{loadSize}')
        self.initialized = True
        return parser

    def gather_options(self):
        # initialize parser with basic options
        if not self.initialized:
            parser = argparse.ArgumentParser(
                formatter_class=argparse.ArgumentDefaultsHelpFormatter)
            parser = self.initialize(parser)

        # get the basic options
        opt, _ = parser.parse_known_args()
        self.parser = parser

        return parser.parse_args()

    def print_options(self, opt):
        message = ''
        message += '----------------- Options ---------------\n'
        for k, v in sorted(vars(opt).items()):
            comment = ''
            default = self.parser.get_default(k)
            if v != default:
                comment = '\t[default: %s]' % str(default)
            message += '{:>25}: {:<30}{}\n'.format(str(k), str(v), comment)
        message += '----------------- End -------------------'
        print(message)

        # save to the disk
        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)
        util.mkdirs(expr_dir)
        file_name = os.path.join(expr_dir, 'opt.txt')
        with open(file_name, 'wt') as opt_file:
            opt_file.write(message)
            opt_file.write('\n')

    def parse(self, print_options=True):

        opt = self.gather_options()
        opt.isTrain = self.isTrain   # train or test

        # process opt.suffix
        if opt.suffix:
            suffix = ('_' + opt.suffix.format(**vars(opt))) if opt.suffix != '' else ''
            opt.name = opt.name + suffix

        if print_options:
            self.print_options(opt)

        # set gpu ids
        str_ids = opt.gpu_ids.split(',')
        opt.gpu_ids = []
        for str_id in str_ids:
            id = int(str_id)
            if id >= 0:
                opt.gpu_ids.append(id)
        if len(opt.gpu_ids) > 0:
            torch.cuda.set_device(opt.gpu_ids[0])

        # additional
        opt.classes = opt.classes.split(',')
        # opt.rz_interp = opt.rz_interp.split(',')
        opt.blur_sig = [float(s) for s in opt.blur_sig.split(',')]
        opt.jpg_method = opt.jpg_method.split(',')
        opt.jpg_qual = [int(s) for s in opt.jpg_qual.split(',')]
        if len(opt.jpg_qual) == 2:
            opt.jpg_qual = list(range(opt.jpg_qual[0], opt.jpg_qual[1] + 1))
        elif len(opt.jpg_qual) > 2:
            raise ValueError("Shouldn't have more than 2 values for --jpg_qual.")

        self.opt = opt
        return self.opt


  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\options\test_options.py
     文件内容:
  from .base_options import BaseOptions


class TestOptions(BaseOptions):
    def initialize(self, parser):
        parser = BaseOptions.initialize(self, parser)
        parser.add_argument('--model_path')
        parser.add_argument('--no_resize', action='store_true')
        parser.add_argument('--no_crop', action='store_true')
        parser.add_argument('--eval', action='store_true', help='use eval mode during test time.')
        parser.add_argument('--results_dir', default='./results/')

        self.isTrain = False
        self.isTest = True
        return parser


  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\options\train_options.py
     文件内容:
  from .base_options import BaseOptions


class TrainOptions(BaseOptions):
    def initialize(self, parser):
        parser = BaseOptions.initialize(self, parser)
        parser.add_argument('--earlystop_epoch', type=int, default=5)
        parser.add_argument('--data_aug', action='store_true', help='if specified, perform additional data augmentation (photometric, blurring, jpegging)')
        parser.add_argument('--optim', type=str, default='adam', help='optim to use [sgd, adam]')
        parser.add_argument('--new_optim', action='store_true', help='new optimizer instead of loading the optim state')
        parser.add_argument('--loss_freq', type=int, default=400, help='frequency of showing loss on tensorboard')
        parser.add_argument('--save_latest_freq', type=int, default=2000, help='frequency of saving the latest results')
        parser.add_argument('--save_epoch_freq', type=int, default=20, help='frequency of saving checkpoints at the end of epochs')
        parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')
        parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')
        parser.add_argument('--last_epoch', type=int, default=-1, help='starting epoch count for scheduler intialization')
        parser.add_argument('--train_split', type=str, default='train', help='train, val, test, etc')
        parser.add_argument('--val_split', type=str, default='val', help='train, val, test, etc')
        parser.add_argument('--niter', type=int, default=10000, help='# of iter at starting learning rate')
        parser.add_argument('--beta1', type=float, default=0.9, help='momentum term of adam')
        parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate for adam')

        self.isTrain = True
        return parser


  └── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\options\__init__.py
     文件内容:
  

└── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\README.md
   文件内容:
# Diffusion Noise Feature: Accurate and Fast Generated Image Detection

[Yichi Zhang](https://yichics.github.io/) and [Xiaogang Xu](https://xiaogang00.github.io/)

Code repository for the paper: [Diffusion Noise Feature: Accurate and Fast Generated Image Detection](https://arxiv.org/abs/2312.02625v2). 

![fig](fig/fig1.png)

### Baseline

The code is based on [CNNDetction](https://github.com/PeterWang512/CNNDetection)


**Model Preparation**

The new checkpoints will release soon.

Download the LSUN Bedroom pretrained DDIM from [here](https://heibox.uni-heidelberg.de/f/f179d4f21ebc4d43bbfe/?dl=1). Place it in `./weights/diffusion/` .

**Dataset Preparation**

The testsets can be download from [Huggingface](https://huggingface.co/datasets/toru0035/DNFTestSet).
Download the **DiffusionForensics** from  [DIRE](https://github.com/ZhendongWang6/DIRE). 

Please refer to [CNNDetction](https://github.com/PeterWang512/CNNDetection) for the storage path of the dataset.

**Transform Image to DNF**

```
python compute_dnf.py
```

**Training**

```
python train.py 
```
**Evaluation**

```
python eval.py 
```


Please refer to `./options` for variables that determine the program’s execution.


### Citation 

```
@article{zhang2023diffusion,
  title={Diffusion noise feature: Accurate and fast generated image detection},
  author={Zhang, Yichi and Xu, Xiaogang},
  journal={arXiv preprint arXiv:2312.02625},
  year={2023}
}
```


└── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\requirements.txt
   文件内容:
scipy
scikit-learn
numpy
opencv_python
Pillow
torch>=1.2.0
torchvision


└── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\train.py
   文件内容:
import os
import sys
import time
import torch
import torch.nn
import argparse
from PIL import Image
from tensorboardX import SummaryWriter

from time import time
from validate import validate
from data import create_dataloader
from earlystop import EarlyStopping
from networks.trainer import Trainer
from options.train_options import TrainOptions


"""Currently assumes jpg_prob, blur_prob 0 or 1"""
def get_val_opt():
    val_opt = TrainOptions().parse(print_options=False)
    val_opt.dataroot = '{}/{}/'.format(val_opt.dataroot, val_opt.val_split)
    val_opt.isTrain = False
    val_opt.no_resize = False
    val_opt.no_crop = False
    val_opt.serial_batches = True
    val_opt.jpg_method = ['pil']
    if len(val_opt.blur_sig) == 2:
        b_sig = val_opt.blur_sig
        val_opt.blur_sig = [(b_sig[0] + b_sig[1]) / 2]
    if len(val_opt.jpg_qual) != 1:
        j_qual = val_opt.jpg_qual
        val_opt.jpg_qual = [int((j_qual[0] + j_qual[-1]) / 2)]

    return val_opt


if __name__ == '__main__':
    import torch.multiprocessing as mp
    mp.set_start_method(method='forkserver', force=True)
    
    
    opt = TrainOptions().parse()
    opt.dataroot = '{}/{}/'.format(opt.dataroot, opt.train_split)
    val_opt = get_val_opt()

    data_loader = create_dataloader(opt)
    dataset_size = len(data_loader)
    print('#training images = %d' % dataset_size)

    train_writer = SummaryWriter(os.path.join(opt.checkpoints_dir, opt.name, "train"))
    val_writer = SummaryWriter(os.path.join(opt.checkpoints_dir, opt.name, "val"))

    model = Trainer(opt)
    early_stopping = EarlyStopping(patience=opt.earlystop_epoch, delta=-0.001, verbose=True)
    from tqdm import tqdm
    for epoch in range(opt.niter):
        epoch_iter = 0

        # t1 = time()
        for i, data in enumerate(tqdm(data_loader)):
            
            # t0 = time()
            model.total_steps += 1
            epoch_iter += opt.batch_size

            model.set_input(data)
            model.optimize_parameters()

            if model.total_steps % opt.loss_freq == 0:
                print("Train loss: {} at step: {}".format(model.loss, model.total_steps))
                train_writer.add_scalar('loss', model.loss, model.total_steps)

            if model.total_steps % opt.save_latest_freq == 0:
                print('saving the latest model %s (epoch %d, model.total_steps %d)' %
                      (opt.name, epoch, model.total_steps))
                model.save_networks('latest')
            
            # print(time()-t0, time()-t1)
            # t1 = time()


        if epoch % opt.save_epoch_freq == 0:
            print('saving the model at the end of epoch %d, iters %d' %
                  (epoch, model.total_steps))
            model.save_networks('latest')
            model.save_networks(epoch)

        # Validation
        model.eval()
        acc, ap = validate(model.model, val_opt)[:2]
        val_writer.add_scalar('accuracy', acc, model.total_steps)
        val_writer.add_scalar('ap', ap, model.total_steps)
        print("(Val @ epoch {}) acc: {}; ap: {}".format(epoch, acc, ap))

        early_stopping(acc, model)
        if early_stopping.early_stop:
            cont_train = model.adjust_learning_rate()
            if cont_train:
                print("Learning rate dropped by 10, continue training...")
                early_stopping = EarlyStopping(patience=opt.earlystop_epoch, delta=-0.002, verbose=True)
            else:
                print("Early stopping.")
                break
        model.train()



└── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\util.py
   文件内容:
import os
import torch


def mkdirs(paths):
    if isinstance(paths, list) and not isinstance(paths, str):
        for path in paths:
            mkdir(path)
    else:
        mkdir(paths)


def mkdir(path):
    if not os.path.exists(path):
        os.makedirs(path)


def unnormalize(tens, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):
    # assume tensor of shape NxCxHxW
    return tens * torch.Tensor(std)[None, :, None, None] + torch.Tensor(
        mean)[None, :, None, None]


└── 文件路径: S:\Notes\HFUT\Experiment\_228\Diffusion-Noise-Feature\validate.py
   文件内容:
import torch
import numpy as np
from networks.resnet import resnet50
from sklearn.metrics import average_precision_score, precision_recall_curve, accuracy_score
from options.test_options import TestOptions
from data import create_dataloader

from tqdm import tqdm


def validate(model, opt):
    data_loader = create_dataloader(opt)
    if opt.num_classes > 1:
        tacc = 0.0
        with torch.no_grad():
            for img, label in tqdm(data_loader):
                # img = img.to(opt.device)
                img = img.cuda()
                pred = model(img)
                _, pred = torch.max(pred.data, dim=1)
                tacc += pred.cpu().eq(label).sum()
                # print(pred, label, tacc)
                # exit()
        return tacc/len(data_loader), 0, 0, 0, 0, 0

    
    else:

        with torch.no_grad():
            y_true, y_pred = [], []
            for img, label in tqdm(data_loader):
                in_tens = img.cuda()
                if opt.num_classes > 1:
                    y_pred.extend(model(in_tens).softmax().flatten().tolist())
                else:
                    y_pred.extend(model(in_tens).sigmoid().flatten().tolist())
                y_true.extend(label.flatten().tolist())

        y_true, y_pred = np.array(y_true), np.array(y_pred)
        r_acc = accuracy_score(y_true[y_true==0], y_pred[y_true==0] > 0.5)
        f_acc = accuracy_score(y_true[y_true==1], y_pred[y_true==1] > 0.5)
        acc = accuracy_score(y_true, y_pred > 0.5)
        ap = average_precision_score(y_true, y_pred)
        return acc, ap, r_acc, f_acc, y_true, y_pred


# if __name__ == '__main__':
#     opt = TestOptions().parse(print_options=False)

#     model = resnet50(num_classes=1)
#     state_dict = torch.load(opt.model_path, map_location='cpu')
#     model.load_state_dict(state_dict['model'])
#     model.cuda()
#     model.eval()

#     acc, avg_precision, r_acc, f_acc, y_true, y_pred = validate(model, opt)

#     print("accuracy:", acc)
#     print("average precision:", avg_precision)

#     print("accuracy of real images:", r_acc)
#     print("accuracy of fake images:", f_acc)


